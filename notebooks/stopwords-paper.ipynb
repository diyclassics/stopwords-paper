{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cltk.corpus.utils.importer import CorpusImporter\n",
    "# corpus_importer = CorpusImporter('latin')\n",
    "# corpus_importer.import_corpus('latin_text_latin_library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import html\n",
    "import re\n",
    "\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CLTK tools\n",
    "\n",
    "word_tokenizer = WordTokenizer('latin')\n",
    "replacer = JVReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2159 files in the Latin Library corpus.\n"
     ]
    }
   ],
   "source": [
    "# Setup files\n",
    "\n",
    "files = latinlibrary.fileids()\n",
    "print(\"There are %d files in the Latin Library corpus.\" % len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 965 files in the Latin Library Classical subcorpus.\n"
     ]
    }
   ],
   "source": [
    "#Filter for classical texts\n",
    "\n",
    "classical = []\n",
    "\n",
    "remove = [\"The Bible\",\"Ius Romanum\",\"Papal Bulls\",\"Medieval Latin\",\"Christian Latin\",\"Christina Latin\",\"Neo-Latin\",\"The Miscellany\",\"Contemporary Latin\"]\n",
    "\n",
    "for file in files:\n",
    "   raw = latinlibrary.raw(file)\n",
    "   if not any(x in raw for x in remove):\n",
    "       classical.append(file)\n",
    "\n",
    "files = classical\n",
    "print(\"There are %d files in the Latin Library Classical subcorpus.\" % len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 138 files in the Latin Library Classical subcorpus.\n",
      "['cicero/acad.txt',\n",
      " 'cicero/adbrutum1.txt',\n",
      " 'cicero/adbrutum2.txt',\n",
      " 'cicero/amic.txt',\n",
      " 'cicero/arch.txt',\n",
      " 'cicero/att1.txt',\n",
      " 'cicero/att10.txt',\n",
      " 'cicero/att11.txt',\n",
      " 'cicero/att12.txt',\n",
      " 'cicero/att13.txt']\n"
     ]
    }
   ],
   "source": [
    "#Filter for Cicero texts\n",
    "\n",
    "cicero = [file for file in latinlibrary.fileids() if 'cicero/' in file]\n",
    "\n",
    "\n",
    "files = cicero\n",
    "print(f\"There are {len(files)} files in the Latin Library Classical subcorpus.\")\n",
    "pprint(files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "def preprocess(text):    \n",
    "\n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = re.sub(r'&nbsp;?', ' ',text) #&nbsp; stripped incorrectly in corpus?\n",
    "    text = re.sub('\\x00',' ',text) #Another space problem?\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j\n",
    "    \n",
    "    punctuation =\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    remove_list = [r'\\bthe latin library\\b',\n",
    "                   r'\\bthe classics page\\b',\n",
    "                   r'\\bneo-latin\\b', \n",
    "                   r'\\bmedieval latin\\b',\n",
    "                   r'\\bchristian latin\\b',\n",
    "                   r'\\bchristina latin\\b',\n",
    "                   r'\\bpapal bulls\\b',\n",
    "                   r'\\bthe miscellany\\b',\n",
    "                  ]\n",
    "\n",
    "    for pattern in remove_list:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    text = re.sub('[ ]+',' ', text) # Remove double spaces\n",
    "    text = re.sub('\\s+\\n+\\s+','\\n', text) # Remove double lines and trim spaces around new lines\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of texts\n",
    "\n",
    "raw_files = []\n",
    "\n",
    "for file in files:\n",
    "    raw = latinlibrary.raw(file)\n",
    "    raw = preprocess(raw)\n",
    "    if len(raw) < 1000:\n",
    "        pass\n",
    "    else:\n",
    "        raw_tokens = raw.split()\n",
    "        raw = \" \".join(raw_tokens[50:-50])\n",
    "        raw_files.append(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_document(document, length):\n",
    "    segments = []\n",
    "    wordlist = document.split()\n",
    "    for i in range(0, len(wordlist), length):\n",
    "        segments.append(wordlist[i:i+length])\n",
    "    segments = [\" \".join(segment) for segment in segments]\n",
    "    if len(wordlist) % length:\n",
    "        segments[-2:] = [' '.join(segments[-2:])]\n",
    "        return segments\n",
    "    else:\n",
    "        return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = [segment_document(file, 500) for file in raw_files]\n",
    "segments = [item for sublist in segments for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "2143\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_files))\n",
    "print(len(segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following [Zou et al. 2006; Alajmi 2012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make document-term matrix and vocabulary\n",
    "\n",
    "vectorizer = CountVectorizer(input='content', min_df=3)\n",
    "dtm = vectorizer.fit_transform(segments)\n",
    "dtm = dtm.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27670\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(vocab)\n",
    "N= len(raw_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array of probabilities per book\n",
    "\n",
    "raw_lengths = [len(tokens.split()) for tokens in segments]\n",
    "l = np.array(raw_lengths)\n",
    "ll = l.reshape(len(l),1)\n",
    "\n",
    "probs = dtm/ll\n",
    "\n",
    "P=probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean probability\n",
    "# i.e. Sum of probabilities for each word / number of documents\n",
    "\n",
    "probsum = np.ravel(probs.sum(axis=0))\n",
    "MP = probsum/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array of bar probability\n",
    "\n",
    "length = sum(raw_lengths)\n",
    "barprobs = dtm/length\n",
    "bP=barprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = (P-bP) ** 2\n",
    "varsum = np.ravel(variance.sum(axis=0))\n",
    "VP = varsum/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAT = MP/VP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = list(zip(vocab, MP, VP, SAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_data = sorted(table_data,key=lambda x: x[1], reverse=True)\n",
    "table2_data = sorted(table_data,key=lambda x: x[2], reverse=True)\n",
    "table3_data = sorted(table_data,key=lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['et', 'in', 'non', 'est', 'ut', 'cum', 'quod', 'ad', 'qui', 'esse']\n",
      "['and', 'in', 'is', 'not', 'when/with', 'so/how', 'at', 'which/because', 'who', 'but']\n",
      "['0.3651', '0.3195', '0.2426', '0.2076', '0.1974', '0.1596', '0.1477', '0.1374', '0.1309', '0.1289']\n",
      "['0.0103', '0.0076', '0.0046', '0.0037', '0.0032', '0.0021', '0.0019', '0.0017', '0.0016', '0.0015']\n",
      "['35.3682', '41.8374', '52.4949', '55.8152', '62.3797', '74.5576', '77.9300', '80.0493', '81.7099', '84.7584']\n"
     ]
    }
   ],
   "source": [
    "# Get Table1 info in order\n",
    "words = [item[0] for item in table1_data][:10]\n",
    "print(words)\n",
    "\n",
    "# Current output: \n",
    "# ['et', 'in', 'est', 'non', 'cum', 'ut', 'ad', 'quod', 'qui', 'sed']\n",
    "\n",
    "translations = \"and in is not when/with so/how at which/because who but\".split()\n",
    "print(translations)\n",
    "\n",
    "mps = ['{:.4f}'.format(round(item[1], 4)) for item in table1_data][:10]\n",
    "print(mps)\n",
    "\n",
    "vps = ['{:.4f}'.format(round(item[2], 4)) for item in table1_data][:10]\n",
    "print(vps)\n",
    "\n",
    "sats = ['{:.4f}'.format(round(item[3], 4)) for item in table1_data][:10]\n",
    "print(sats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1. Top 10 words with highest MP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Mean Prob.</th>\n",
       "      <th>Var. Prob.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>et</td>\n",
       "      <td>and</td>\n",
       "      <td>0.3651</td>\n",
       "      <td>0.0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>0.3195</td>\n",
       "      <td>0.0076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non</td>\n",
       "      <td>is</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>0.0046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>est</td>\n",
       "      <td>not</td>\n",
       "      <td>0.2076</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ut</td>\n",
       "      <td>when/with</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cum</td>\n",
       "      <td>so/how</td>\n",
       "      <td>0.1596</td>\n",
       "      <td>0.0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quod</td>\n",
       "      <td>at</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ad</td>\n",
       "      <td>which/because</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.0017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>qui</td>\n",
       "      <td>who</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>esse</td>\n",
       "      <td>but</td>\n",
       "      <td>0.1289</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word    Translation Mean Prob. Var. Prob.\n",
       "0    et            and     0.3651     0.0103\n",
       "1    in             in     0.3195     0.0076\n",
       "2   non             is     0.2426     0.0046\n",
       "3   est            not     0.2076     0.0037\n",
       "4    ut      when/with     0.1974     0.0032\n",
       "5   cum         so/how     0.1596     0.0021\n",
       "6  quod             at     0.1477     0.0019\n",
       "7    ad  which/because     0.1374     0.0017\n",
       "8   qui            who     0.1309     0.0016\n",
       "9  esse            but     0.1289     0.0015"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1 = [(word, translation, mp, vp) for word, translation, mp, vp, _ in zip(words, translations, mps, vps, sats)]\n",
    "df1 = pandas.DataFrame(table1, columns=['Word', 'Translation', 'Mean Prob.', 'Var. Prob.'])\n",
    "\n",
    "print(\"Table 1. Top 10 words with highest MP\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['et', 'in', 'non', 'est', 'ut', 'cum', 'quod', 'ad', 'qui', 'si']\n",
      "['and', 'in', 'is', 'not', 'when/with', 'so/how', 'at', 'which/because', 'who', 'if']\n",
      "['0.3651', '0.3195', '0.2426', '0.2076', '0.1974', '0.1596', '0.1477', '0.1374', '0.1309', '0.1282']\n",
      "['0.01032', '0.00764', '0.00462', '0.00372', '0.00316', '0.00214', '0.00189', '0.00172', '0.00160', '0.00160']\n",
      "['35.3682', '41.8374', '52.4949', '55.8152', '62.3797', '74.5576', '77.9300', '80.0493', '81.7099', '80.0770']\n"
     ]
    }
   ],
   "source": [
    "# Get Table2 info in order\n",
    "words2 = [item[0] for item in table2_data][:10]\n",
    "print(words2)\n",
    "\n",
    "# Current output: \n",
    "# ['et', 'in', 'est', 'non', 'cum', 'ut', 'ad', 'quod', 'qui', 'si']\n",
    "\n",
    "translations2 = \"and in is not when/with so/how at which/because who if\".split()\n",
    "print(translations2)\n",
    "\n",
    "mps2 = ['{:.4f}'.format(round(item[1], 4)) for item in table2_data][:10]\n",
    "print(mps2)\n",
    "\n",
    "vps2 = ['{:.5f}'.format(round(item[2], 5)) for item in table2_data][:10]\n",
    "print(vps2)\n",
    "\n",
    "sats2 = ['{:.4f}'.format(round(item[3], 4)) for item in table2_data][:10]\n",
    "print(sats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 2. Top 10 words with lowest VP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Mean Prob.</th>\n",
       "      <th>Var. Prob.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>et</td>\n",
       "      <td>and</td>\n",
       "      <td>0.3651</td>\n",
       "      <td>0.01032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>0.3195</td>\n",
       "      <td>0.00764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non</td>\n",
       "      <td>is</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>0.00462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>est</td>\n",
       "      <td>not</td>\n",
       "      <td>0.2076</td>\n",
       "      <td>0.00372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ut</td>\n",
       "      <td>when/with</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.00316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cum</td>\n",
       "      <td>so/how</td>\n",
       "      <td>0.1596</td>\n",
       "      <td>0.00214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quod</td>\n",
       "      <td>at</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.00189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ad</td>\n",
       "      <td>which/because</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.00172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>qui</td>\n",
       "      <td>who</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>0.00160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>si</td>\n",
       "      <td>if</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.00160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word    Translation Mean Prob. Var. Prob.\n",
       "0    et            and     0.3651    0.01032\n",
       "1    in             in     0.3195    0.00764\n",
       "2   non             is     0.2426    0.00462\n",
       "3   est            not     0.2076    0.00372\n",
       "4    ut      when/with     0.1974    0.00316\n",
       "5   cum         so/how     0.1596    0.00214\n",
       "6  quod             at     0.1477    0.00189\n",
       "7    ad  which/because     0.1374    0.00172\n",
       "8   qui            who     0.1309    0.00160\n",
       "9    si             if     0.1282    0.00160"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2 = [(word, translation, mp, vp) for word, translation, mp, vp, _ in zip(words2, translations2, mps2, vps2, sats2)]\n",
    "df2 = pandas.DataFrame(table2, columns=['Word', 'Translation', 'Mean Prob.', 'Var. Prob.'])\n",
    "\n",
    "print(\"Table 2. Top 10 words with lowest VP\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['et', 'in', 'non', 'cereris', 'est', 'ut', 'sunto', 'te', 'aut', 'me']\n",
      "['and', 'in', 'is', 'not', 'when/with', 'so/how', 'at', 'which/because', 'who', 'me']\n",
      "['0.3651', '0.3195', '0.2426', '0.0003', '0.2076', '0.1974', '0.0004', '0.0837', '0.0931', '0.0852']\n",
      "['0.01032', '0.00764', '0.00462', '0.00001', '0.00372', '0.00316', '0.00001', '0.00125', '0.00133', '0.00119']\n",
      "['35.3682', '41.8374', '52.4949', '55.7312', '55.8152', '62.3797', '64.6519', '66.9224', '69.7164', '71.7725']\n"
     ]
    }
   ],
   "source": [
    "# Get Table3 info in order\n",
    "words3 = [item[0] for item in table3_data][:10]\n",
    "print(words3)\n",
    "\n",
    "# Current output: \n",
    "# ['et', 'in', 'est', 'non', 'cum', 'ut', 'ad', 'quod', 'qui', 'me']\n",
    "\n",
    "translations3 = \"and in is not when/with so/how at which/because who me\".split()\n",
    "print(translations3)\n",
    "\n",
    "mps3 = ['{:.4f}'.format(round(item[1], 4)) for item in table3_data][:10]\n",
    "print(mps3)\n",
    "\n",
    "vps3 = ['{:.5f}'.format(round(item[2], 5)) for item in table3_data][:10]\n",
    "print(vps3)\n",
    "\n",
    "sats3 = ['{:.4f}'.format(round(item[3], 4)) for item in table3_data][:10]\n",
    "print(sats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3. Top 10 words with highest SAT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Mean Prob.</th>\n",
       "      <th>Var. Prob.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>et</td>\n",
       "      <td>and</td>\n",
       "      <td>0.3651</td>\n",
       "      <td>0.01032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>0.3195</td>\n",
       "      <td>0.00764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non</td>\n",
       "      <td>is</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>0.00462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cereris</td>\n",
       "      <td>not</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>est</td>\n",
       "      <td>when/with</td>\n",
       "      <td>0.2076</td>\n",
       "      <td>0.00372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ut</td>\n",
       "      <td>so/how</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.00316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sunto</td>\n",
       "      <td>at</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>te</td>\n",
       "      <td>which/because</td>\n",
       "      <td>0.0837</td>\n",
       "      <td>0.00125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aut</td>\n",
       "      <td>who</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.00133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>me</td>\n",
       "      <td>me</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>0.00119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word    Translation Mean Prob. Var. Prob.\n",
       "0       et            and     0.3651    0.01032\n",
       "1       in             in     0.3195    0.00764\n",
       "2      non             is     0.2426    0.00462\n",
       "3  cereris            not     0.0003    0.00001\n",
       "4      est      when/with     0.2076    0.00372\n",
       "5       ut         so/how     0.1974    0.00316\n",
       "6    sunto             at     0.0004    0.00001\n",
       "7       te  which/because     0.0837    0.00125\n",
       "8      aut            who     0.0931    0.00133\n",
       "9       me             me     0.0852    0.00119"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3 = [(word, translation, mp, vp) for word, translation, mp, vp, _ in zip(words3, translations3, mps3, vps3, sats3)]\n",
    "df3 = pandas.DataFrame(table3, columns=['Word', 'Translation', 'Mean Prob.', 'Var. Prob.'])\n",
    "\n",
    "print(\"Table 3. Top 10 words with highest SAT\")\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate entropies\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    logprobs = np.where(probs != 0, np.log10(1/probs), 0)\n",
    "ent = probs * logprobs\n",
    "H = np.ravel(ent.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = list(zip(vocab, MP, VP, SAT, H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "table4_data = sorted(table_data,key=lambda x: x[4], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['et', 'in', 'non', 'est', 'ut', 'cum', 'quod', 'ad', 'qui', 'esse']\n",
      "['and', 'in', 'is', 'not', 'when/with', 'so/how', 'at', 'which/because', 'who', 'me']\n"
     ]
    }
   ],
   "source": [
    "# Get Table4 info in order\n",
    "words4 = [item[0] for item in table4_data][:10]\n",
    "print(words4)\n",
    "\n",
    "# Current output: \n",
    "# ['et', 'in', 'est', 'non', 'cum', 'ut', 'ad', 'quod', 'qui', 'me']\n",
    "\n",
    "translations4 = \"and in is not when/with so/how at which/because who me\".split()\n",
    "print(translations3)\n",
    "\n",
    "#mps3 = ['{:.4f}'.format(round(item[1], 4)) for item in table3_data][:10]\n",
    "#print(mps3)\n",
    "\n",
    "#vps3 = ['{:.5f}'.format(round(item[2], 5)) for item in table3_data][:10]\n",
    "#print(vps3)\n",
    "\n",
    "#sats3 = ['{:.4f}'.format(round(item[3], 4)) for item in table3_data][:10]\n",
    "#print(sats3)\n",
    "\n",
    "ents4 = ['{:.4f}'.format(round(item[4], 4)) for item in table4_data][:10]\n",
    "#print(sats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 4. Top 10 words with highest entropy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>et</td>\n",
       "      <td>and</td>\n",
       "      <td>79.8614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>72.8263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non</td>\n",
       "      <td>is</td>\n",
       "      <td>58.8568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>est</td>\n",
       "      <td>not</td>\n",
       "      <td>51.6368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ut</td>\n",
       "      <td>when/with</td>\n",
       "      <td>50.0964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cum</td>\n",
       "      <td>so/how</td>\n",
       "      <td>42.2736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quod</td>\n",
       "      <td>at</td>\n",
       "      <td>39.6297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ad</td>\n",
       "      <td>which/because</td>\n",
       "      <td>37.1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>qui</td>\n",
       "      <td>who</td>\n",
       "      <td>35.6601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>esse</td>\n",
       "      <td>me</td>\n",
       "      <td>35.3438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word    Translation  Entropy\n",
       "0    et            and  79.8614\n",
       "1    in             in  72.8263\n",
       "2   non             is  58.8568\n",
       "3   est            not  51.6368\n",
       "4    ut      when/with  50.0964\n",
       "5   cum         so/how  42.2736\n",
       "6  quod             at  39.6297\n",
       "7    ad  which/because  37.1852\n",
       "8   qui            who  35.6601\n",
       "9  esse             me  35.3438"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table4 = [(word, translation, ent) for word, translation, ent in zip(words4, translations4, ents4)]\n",
    "df4 = pandas.DataFrame(table4, columns=['Word', 'Translation', 'Entropy'])\n",
    "\n",
    "print(\"Table 4. Top 10 words with highest entropy\")\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.stop.stop import CorpusStoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CorpusStoplist('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method build_stoplist in module cltk.stop.stop:\n",
      "\n",
      "build_stoplist(texts, basis='zou', size=100, sort_words=True, inc_counts=False, lower=True, remove_punctuation=True, remove_numbers=True, include=[], exclude=[]) method of cltk.stop.stop.CorpusStoplist instance\n",
      "    :param texts: list of strings used as document collection for extracting stopwords\n",
      "    :param basis: Define the basis for extracting stopwords from the corpus. Available methods are:\n",
      "                  - 'frequency', word counts\n",
      "                  - 'mean', mean probabilities\n",
      "                  - 'variance', variance probabilities\n",
      "                  - 'entropy', entropy\n",
      "                  - 'zou', composite measure as defined in the following paper\n",
      "                    Zou, F., Wang, F.L., Deng, X., Han, S., and Wang, L.S. 2006. “Automatic Construction of Chinese Stop Word List.” In Proceedings of the 5th WSEAS International Conference on Applied Computer Science, 1010–1015. https://pdfs.semanticscholar.org/c543/8e216071f6180c228cc557fb1d3c77edb3a3.pdf.\n",
      "    :param size: Set the size of the output list\n",
      "    :param sort_words: Sort output list alphabetically? (Otherwise return is descending by basis value)        \n",
      "    :param inc_counts: Include basis value; e.g. word counts for 'frequency', mean probabilities for 'mean'\n",
      "    :param lower: Lowercase corpus or no?\n",
      "    :param remove_punctuation: Remove punctuation from corpus or no?\n",
      "    :param remove_numbers: Remove numbers from corpus or no?\n",
      "    :param include: List of words in addition to stopwords that are extracted from the document collection\n",
      "                    to be added to the final list  \n",
      "    :param exclude: List of words in addition to stopwords that are extracted from the document collection\n",
      "                    to be removed from the final list \n",
      "    :type texts: list\n",
      "    :type basis: str\n",
      "    :type size: int\n",
      "    :type sort_words: bool\n",
      "    :type inc_counts: bool\n",
      "    :type lower: bool\n",
      "    :type remove_punctuation: bool\n",
      "    :type remove_numbers: bool\n",
      "    :type include: list\n",
      "    :type exclude: list\n",
      "    :return: a list of stopwords extracted from the corpus\n",
      "    :rtype: list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(c.build_stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_stops = c.build_stoplist(segments, basis='frequency', inc_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'an', 'ante', 'apud', 'atque', 'aut', 'autem', 'causa', 'cum', 'de', 'ea', 'ego', 'eius', 'enim', 'eo', 'erat', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex']\n"
     ]
    }
   ],
   "source": [
    "print(freq_stops[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table: Word Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.stop.stop import CorpusStoplist\n",
    "\n",
    "import pickle\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_files = latinlibrary.fileids()\n",
    "ll_docs = [latinlibrary.raw(file) for file in ll_files]\n",
    "ll_size = len(ll_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "import html\n",
    "import re\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "\n",
    "replacer = JVReplacer()\n",
    "\n",
    "def preprocess(text):    \n",
    "\n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = re.sub(r'&nbsp;?', ' ',text) #&nbsp; stripped incorrectly in corpus?\n",
    "#     text = re.sub('\\x00',' ',text) #Another space problem?\n",
    "    \n",
    "#     text = text.lower()\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j\n",
    "\n",
    "    remove_list = [r'\\bthe latin library\\b',\n",
    "                   r'\\bthe classics page\\b',\n",
    "                   r'\\bneo-latin\\b', \n",
    "                   r'\\bmedieval latin\\b',\n",
    "                   r'\\bchristian latin\\b',\n",
    "                   r'\\bchristina latin\\b',\n",
    "                   r'\\bpapal bulls\\b',\n",
    "                   r'\\bthe miscellany\\b',\n",
    "                  ]\n",
    "\n",
    "    for pattern in remove_list:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    text = re.sub('[ ]+',' ', text) # Remove double spaces\n",
    "    text = re.sub('\\s+\\n+\\s+','\\n', text) # Remove double lines and trim spaces around new lines\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cicero_files = [file for file in latinlibrary.fileids() if 'cicero/' in file]\n",
    "cicero_docs = [preprocess(latinlibrary.raw(file)) for file in cicero_files]\n",
    "cicero_size = len(cicero_files)\n",
    "\n",
    "biblia_sacra_files = [file for file in latinlibrary.fileids() if 'bible/' in file]\n",
    "biblia_sacra_docs = [preprocess(latinlibrary.raw(file)) for file in biblia_sacra_files]\n",
    "biblia_sacra_size = len(biblia_sacra_files)\n",
    "\n",
    "ius_romanum_files = [file for file in latinlibrary.fileids() if 'justinian' in file \n",
    "                     or 'gaius' in file \n",
    "                     or 'theod' in file]\n",
    "ius_romanum_docs = [preprocess(latinlibrary.raw(file)) for file in ius_romanum_files]\n",
    "ius_romanum_size = len(ius_romanum_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get tokens for all document collections\n",
    "## NB: Pickled to save loading time\n",
    "\n",
    "# ll_tokens = latinlibrary.words(ll_files)\n",
    "# cicero_tokens = latinlibrary.words(cicero_files)\n",
    "# biblia_sacra_tokens = latinlibrary.words(biblia_sacra_files)\n",
    "# ius_romanum_tokens = latinlibrary.words(ius_romanum_files)\n",
    "\n",
    "# pickle.dump(list(ll_tokens), open('../data/serial/ll_tokens.p', 'wb'))\n",
    "# pickle.dump(list(cicero_tokens), open('../data/serial/cicero_tokens.p', 'wb'))\n",
    "# pickle.dump(list(biblia_sacra_tokens), open('../data/serial/biblia_sacra_tokens.p', 'wb'))\n",
    "# pickle.dump(list(ius_romanum_tokens), open('../data/serial/ius_romanum_tokens.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_tokens = pickle.load(open('../data/serial/ll_tokens.p', 'rb'))\n",
    "cicero_tokens = pickle.load(open('../data/serial/cicero_tokens.p', 'rb'))\n",
    "biblia_sacra_tokens = pickle.load(open('../data/serial/biblia_sacra_tokens.p', 'rb'))\n",
    "ius_romanum_tokens = pickle.load(open('../data/serial/ius_romanum_tokens.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_words_size = len(ll_tokens)\n",
    "cicero_words_size = len(cicero_tokens)\n",
    "biblia_sacra_words_size = len(biblia_sacra_tokens)\n",
    "ius_romanum_words_size = len(ius_romanum_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpora</th>\n",
       "      <th>No. of Files</th>\n",
       "      <th>No. of Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Latin Library</td>\n",
       "      <td>2164</td>\n",
       "      <td>16656639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cicero</td>\n",
       "      <td>140</td>\n",
       "      <td>1395928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Biblia Sacra</td>\n",
       "      <td>77</td>\n",
       "      <td>708851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ius Romanum</td>\n",
       "      <td>88</td>\n",
       "      <td>2191932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Corpora  No. of Files  No. of Words\n",
       "0  Latin Library          2164      16656639\n",
       "1         Cicero           140       1395928\n",
       "2   Biblia Sacra            77        708851\n",
       "3    Ius Romanum            88       2191932"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora = ['Latin Library', 'Cicero', 'Biblia Sacra', 'Ius Romanum']\n",
    "sizes = [ll_size, cicero_size, biblia_sacra_size, ius_romanum_size]\n",
    "words_sizes = [ll_words_size, cicero_words_size, biblia_sacra_words_size, ius_romanum_words_size]\n",
    "data = {'Corpora': corpora, 'No. of Files': sizes, 'No. of Words': words_sizes}\n",
    "pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CorpusStoplist('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_stops = c.build_stoplist(ll_docs, basis='zou', inc_counts=True)\n",
    "cicero_stops = c.build_stoplist(cicero_docs, basis='zou', inc_counts=True)\n",
    "biblia_sacra_stops = c.build_stoplist(biblia_sacra_docs, basis='zou', inc_counts=True)\n",
    "ius_romanum_stops = c.build_stoplist(ius_romanum_docs, basis='zou', inc_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'atque', 'aut', 'autem', 'bellum', 'classics', 'contra', 'cum', 'de', 'dei', 'deus', 'dum', 'ea', 'ego', 'eius', 'enim', 'eo', 'erat', 'ergo', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuit', 'haec', 'hic', 'his', 'hoc', 'iam', 'id', 'illa', 'ille', 'in', 'inter', 'ipse', 'ita', 'latin', 'liber', 'library', 'livy', 'lt', 'me', 'mihi', 'modo', 'nam', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'page', 'per', 'periocha', 'post', 'pro', 'qua', 'quae', 'quam', 'quem', 'qui', 'quia', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quoque', 'res', 'se', 'secundum', 'sed', 'semper', 'si', 'sibi', 'sic', 'sicut', 'sit', 'sub', 'sunt', 'tamen', 'te', 'the', 'tibi', 'tu', 'ubi', 'urbe', 'ut', 'vel', 'vero']\n"
     ]
    }
   ],
   "source": [
    "print(ll_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'an', 'ante', 'atque', 'aut', 'autem', 'causa', 'cicero', 'cum', 'de', 'ea', 'ego', 'eius', 'enim', 'eo', 'erat', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuit', 'haec', 'hic', 'hoc', 'iam', 'id', 'igitur', 'iis', 'illa', 'ille', 'illud', 'in', 'ipse', 'is', 'ita', 'liber', 'me', 'mihi', 'modo', 'nam', 'natura', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'omnibus', 'omnium', 'per', 'potest', 'pro', 'publica', 'publicae', 'qua', 'quae', 'quam', 'quem', 'qui', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quos', 're', 'rebus', 'rei', 'rem', 'res', 'se', 'sed', 'senatus', 'si', 'sic', 'sine', 'sit', 'sunt', 'tam', 'tamen', 'te', 'the', 'tibi', 'tu', 'tum', 'uel', 'uero', 'uos', 'ut']\n"
     ]
    }
   ],
   "source": [
    "print(cicero_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ad', 'ait', 'autem', 'caput', 'christi', 'christo', 'cum', 'de', 'dei', 'deo', 'deum', 'deus', 'dicit', 'die', 'dixit', 'domine', 'domini', 'domino', 'dominum', 'dominus', 'eam', 'ecce', 'ego', 'ei', 'eis', 'eius', 'enim', 'eo', 'eorum', 'eos', 'erat', 'ergo', 'erit', 'est', 'et', 'eum', 'ex', 'filii', 'filius', 'fratres', 'haec', 'hierusalem', 'hoc', 'iesu', 'in', 'israhel', 'me', 'mea', 'meum', 'mihi', 'ne', 'nec', 'neque', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omni', 'omnia', 'omnibus', 'omnis', 'per', 'pro', 'propter', 'quae', 'quam', 'quasi', 'quem', 'qui', 'quia', 'quid', 'quis', 'quod', 'quoniam', 'rex', 'se', 'secundum', 'sed', 'si', 'sicut', 'suis', 'sum', 'sunt', 'super', 'suum', 'te', 'terra', 'terram', 'the', 'tibi', 'tu', 'tua', 'tui', 'tuum', 'usque', 'ut', 'vobis', 'vos']\n"
     ]
    }
   ],
   "source": [
    "print(biblia_sacra_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'ab', 'ac', 'actio', 'actionem', 'ad', 'an', 'aut', 'autem', 'causa', 'cj', 'conss', 'constantinopoli', 'contra', 'cth', 'cum', 'dat', 'de', 'debet', 'dig', 'ea', 'eam', 'ed', 'ei', 'eius', 'enim', 'eo', 'eorum', 'eos', 'erit', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuerit', 'gt', 'hereditatem', 'heres', 'his', 'hoc', 'id', 'idem', 'imperator', 'imperatores', 'in', 'is', 'ita', 'iulianus', 'iure', 'ius', 'kal', 'lt', 'mihi', 'nam', 'ne', 'nec', 'neque', 'nisi', 'nomine', 'non', 'paulus', 'per', 'posse', 'possit', 'post', 'potest', 'pp', 'pr', 'pro', 'quae', 'quam', 'qui', 'quia', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quoque', 'rei', 'rem', 'res', 'sab', 'se', 'sed', 'si', 'sibi', 'sine', 'sit', 'sive', 'sunt', 'tamen', 'ulpianus', 'ut', 'vel', 'vero']\n"
     ]
    }
   ],
   "source": [
    "print(ius_romanum_stops)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
