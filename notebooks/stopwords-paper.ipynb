{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Stoplists for Historical Languages\n",
    "Code repository associated with the Constructing Stoplists for Historical Languages for Digital Classics Online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cltk.corpus.utils.importer import CorpusImporter\n",
    "# corpus_importer = CorpusImporter('latin')\n",
    "# corpus_importer.import_corpus('latin_text_latin_library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "from cltk.stop.latin import CorpusStoplist\n",
    "from cltk.stop.latin import PERSEUS_STOPS\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing texts\n",
    "\n",
    "def truncate_text(text):\n",
    "    temp = text[500:-500]\n",
    "    start = temp.find(' ')\n",
    "    end = temp.rfind(' ')\n",
    "    return temp[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "import html\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "\n",
    "replacer = JVReplacer()\n",
    "\n",
    "def preprocess(text):    \n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2152 files in the CLTK Latin Library corpus.\n"
     ]
    }
   ],
   "source": [
    "# Load CLTK Latin Library corpus; get size\n",
    "\n",
    "ll_files = latinlibrary.fileids()\n",
    "ll_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in ll_files]\n",
    "ll_docs = [doc for doc in ll_docs if len(doc) > 100]\n",
    "ll_size = len(ll_files)\n",
    "\n",
    "# CITED IN ARTICLE\n",
    "print(f'There are {ll_size} files in the CLTK Latin Library corpus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16287634 tokens in the CLTK Latin Library corpus.\n",
      "There are 482172 unique tokens in the CLTK Latin Library corpus.\n",
      "Of the tokens appearing in the CLTK Latin Library corpus, 225612 tokens appear once.\n"
     ]
    }
   ],
   "source": [
    "# Get tokens for Latin Library; get stats\n",
    "\n",
    "ll_tokens = [WordPunctTokenizer().tokenize(doc) for doc in ll_docs]\n",
    "ll_tokens = [item for sublist in ll_tokens for item in sublist]\n",
    "\n",
    "# CITED IN ARTICLE\n",
    "\n",
    "print(f'There are {len(ll_tokens)} tokens in the CLTK Latin Library corpus.')\n",
    "print(f'There are {len(set(ll_tokens))} unique tokens in the CLTK Latin Library corpus.')\n",
    "print(f'Of the tokens appearing in the CLTK Latin Library corpus, {len([k for k, v in Counter(ll_tokens).items() if v == 1 ])} tokens appear once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stoplist instance\n",
    "\n",
    "c = CorpusStoplist('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequency stops for corpora\n",
    "\n",
    "ll_freq_stops = c.build_stoplist(ll_docs, size=25, basis='frequency', inc_values=True, sort_words=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Results for Different Stoplist Construction \"Bases\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-75f9c9991e1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mll_mean_stops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_stoplist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mll_variance_stops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_stoplist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'variance'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mll_entropy_stops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_stoplist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mll_zou_stops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_stoplist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mll_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zou'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/stopwords-paper-M0fBybxs/src/cltk/cltk/stop/stop.py\u001b[0m in \u001b[0;36mbuild_stoplist\u001b[0;34m(self, texts, basis, size, sort_words, inc_values, lower, remove_punctuation, remove_numbers, include, exclude)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_punctuation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_punctuation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpunctuation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mremove_numbers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/stopwords-paper-M0fBybxs/src/cltk/cltk/stop/stop.py\u001b[0m in \u001b[0;36m_remove_punctuation\u001b[0;34m(self, texts, punctuation)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mpunctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/stopwords-paper-M0fBybxs/src/cltk/cltk/stop/stop.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mpunctuation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mtranslator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaketrans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\" \"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get stoplists for different bases\n",
    "\n",
    "ll_mean_stops = c.build_stoplist(ll_docs, size=500, basis='mean', inc_values=True, sort_words=False)\n",
    "ll_variance_stops = c.build_stoplist(ll_docs, size=500, basis='variance', inc_values=True, sort_words=False)\n",
    "ll_entropy_stops = c.build_stoplist(ll_docs, size=500, basis='entropy', inc_values=True, sort_words=False)\n",
    "ll_zou_stops = c.build_stoplist(ll_docs, size=100, basis='zou', inc_values=True, sort_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant figures for 'zou' derived list\n",
    "\n",
    "ll_zou_words = list(zip(*ll_zou_stops))[0]\n",
    "ll_zou_mean = [round(dict(ll_mean_stops)[word], 4) for word in ll_zou_words]\n",
    "ll_zou_variance = [round(dict(ll_variance_stops)[word], 6) for word in ll_zou_words]\n",
    "ll_zou_entropy = [round(dict(ll_entropy_stops)[word], 4) for word in ll_zou_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print table\n",
    "\n",
    "data = {\n",
    "    'LL \\'Zou\\' Stopwords': ll_zou_words,\n",
    "    'Mean Prob.': ll_zou_mean,\n",
    "    'Var. Prob.': ll_zou_variance,\n",
    "    'Entropy': ll_zou_entropy,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.index += 1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix B: Comparison of Different Latin Stoplists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Perseus stoplist\n",
    "\n",
    "print(PERSEUS_STOPS)\n",
    "print(f'The Perseus stoplist has {len(PERSEUS_STOPS)} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show 100-word LL 'Zou' stoplist\n",
    "\n",
    "ll_stops = c.build_stoplist(ll_docs, size=100, basis='zou')\n",
    "print(ll_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show intersection of Perseus & LL\n",
    "\n",
    "perseus_intersection = set(PERSEUS_STOPS).intersection(set(ll_stops))\n",
    "print(f'There are {len(perseus_intersection)} words shared by the two lists. This amounts to {(len(perseus_intersection)/len(PERSEUS_STOPS))*100}% of the Perseus list.')\n",
    "print(sorted(perseus_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show difference of Perseus & LL\n",
    "\n",
    "perseus_difference = set(ll_stops).difference(set(PERSEUS_STOPS))\n",
    "print(f'There are {len(perseus_intersection)} words from the LL list that are not found on the Perseus list.')\n",
    "print(sorted(perseus_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show difference of Perseus & LL\n",
    "\n",
    "perseus_difference = set(PERSEUS_STOPS).difference(set(ll_stops))\n",
    "print(f'There are {len(perseus_intersection)} words from the Perseus list that are not found on the LL list.')\n",
    "print(sorted(perseus_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show stopwords-json list\n",
    "\n",
    "json_stops = [\"a\",\"ab\",\"ac\",\"ad\",\"at\",\"atque\",\"aut\",\"autem\",\"cum\",\"de\",\"dum\",\"e\",\"erant\",\"erat\",\"est\",\"et\",\"etiam\",\"ex\",\"haec\",\"hic\",\"hoc\",\"in\",\"ita\",\"me\",\"nec\",\"neque\",\"non\",\"per\",\"qua\",\"quae\",\"quam\",\"qui\",\"quibus\",\"quidem\",\"quo\",\"quod\",\"re\",\"rebus\",\"rem\",\"res\",\"sed\",\"si\",\"sic\",\"sunt\",\"tamen\",\"tandem\",\"te\",\"ut\",\"vel\"]\n",
    "\n",
    "print(json_stops)\n",
    "print(f'The stopwords-json stoplist has {len(json_stops)} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show intersection of stopwords-json & LL\n",
    "\n",
    "json_intersection = set(json_stops).intersection(set(ll_stops))\n",
    "print(f'There are {len(json_intersection)} words shared by the two lists. This amounts to {(len(json_intersection)/len(json_stops))*100}% of the stopwords-json list.')\n",
    "print(json_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show difference of stopwords-json & LL\n",
    "\n",
    "json_difference = set(ll_stops).difference(set(json_stops))\n",
    "print(f'There are {len(json_intersection)} words from the LL list that are not found on the stopwords-json list.')\n",
    "print(json_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show difference of stopwords-json & LL\n",
    "\n",
    "json_difference = set(json_stops).difference(set(ll_stops))\n",
    "print(f'There are {len(json_intersection)} words from the stopwords-json list that are not found on the LL list.')\n",
    "print(json_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/serial/voyant.p', 'rb') as f:\n",
    "    voyant_stops = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The Voyant Tools stoplist has {len(voyant_stops)} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show intersection of Voyant Tools & LL\n",
    "\n",
    "voyant_intersection = set(voyant_stops).intersection(set(ll_stops))\n",
    "print(len(voyant_intersection))\n",
    "print(voyant_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show difference of Voyant Tools & LL\n",
    "\n",
    "voyant_difference = set(ll_stops).difference(set(voyant_stops))\n",
    "print(len(voyant_difference))\n",
    "print(voyant_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show difference of Voyant Tools & LL (selection)\n",
    "\n",
    "voyant_difference = set(voyant_stops).difference(set(ll_stops))\n",
    "print(len(voyant_difference))\n",
    "print(sorted(voyant_difference)[:100])\n",
    "print(sorted(voyant_difference)[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix C: 100-Word Stoplists for various Latin corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subcorpora of LL corpus\n",
    "\n",
    "# Cicero files/tokens\n",
    "cic_files = [file for file in latinlibrary.fileids() if 'cicero/' in file]\n",
    "cic_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in cic_files]\n",
    "cic_tokens = [WordPunctTokenizer().tokenize(doc) for doc in cic_docs]\n",
    "cic_tokens = [item for sublist in cic_tokens for item in sublist] #flatten\n",
    "\n",
    "# Biblia Sacra files/tokens\n",
    "bib_files = [file for file in latinlibrary.fileids() if 'bible/' in file]\n",
    "bib_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in bib_files]\n",
    "bib_tokens = [WordPunctTokenizer().tokenize(doc) for doc in bib_docs]\n",
    "bib_tokens = [item for sublist in bib_tokens for item in sublist] #flatten\n",
    "\n",
    "# Roman Legal Texts files/tokens\n",
    "ius_files = [file for file in latinlibrary.fileids() if 'justinian' in file \n",
    "                     or 'gaius' in file \n",
    "                     or 'theod' in file]\n",
    "ius_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in ius_files]\n",
    "ius_tokens = [WordPunctTokenizer().tokenize(doc) for doc in ius_docs]\n",
    "ius_tokens = [item for sublist in ius_tokens for item in sublist] #flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LL 'zou' stoplist\n",
    "\n",
    "ll_stops = c.build_stoplist(ll_docs, size=100, basis='zou')\n",
    "print(ll_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LL-Cic 'zou' stoplist\n",
    "\n",
    "ll_cic_stops = c.build_stoplist(cic_docs, size=100, basis='zou')\n",
    "print(ll_cic_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LL-Bib 'zou' stoplist\n",
    "\n",
    "ll_bib_stops = c.build_stoplist(bib_docs, size=100, basis='zou')\n",
    "print(ll_bib_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LL-Ius 'zou' stoplist\n",
    "\n",
    "ll_ius_stops = c.build_stoplist(ius_docs, size=100, basis='zou')\n",
    "print(ll_ius_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
