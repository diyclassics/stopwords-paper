{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cltk.corpus.utils.importer import CorpusImporter\n",
    "# corpus_importer = CorpusImporter('latin')\n",
    "# corpus_importer.import_corpus('latin_text_latin_library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import html\n",
    "import re\n",
    "\n",
    "import pandas\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "from cltk.tokenize.word import WordTokenizer\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup CLTK tools\n",
    "\n",
    "word_tokenizer = WordTokenizer('latin')\n",
    "replacer = JVReplacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2164 files in the Latin Library corpus.\n"
     ]
    }
   ],
   "source": [
    "# Setup files\n",
    "\n",
    "files = latinlibrary.fileids()\n",
    "print(\"There are %d files in the Latin Library corpus.\" % len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 970 files in the Latin Library Classical subcorpus.\n"
     ]
    }
   ],
   "source": [
    "#Filter for classical texts\n",
    "\n",
    "classical = []\n",
    "\n",
    "remove = [\"The Bible\",\"Ius Romanum\",\"Papal Bulls\",\"Medieval Latin\",\"Christian Latin\",\"Christina Latin\",\"Neo-Latin\",\"The Miscellany\",\"Contemporary Latin\"]\n",
    "\n",
    "for file in files:\n",
    "   raw = latinlibrary.raw(file)\n",
    "   if not any(x in raw for x in remove):\n",
    "       classical.append(file)\n",
    "\n",
    "files = classical\n",
    "print(\"There are %d files in the Latin Library Classical subcorpus.\" % len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 138 files in the Latin Library Classical subcorpus.\n",
      "['cicero/acad.txt',\n",
      " 'cicero/adbrutum1.txt',\n",
      " 'cicero/adbrutum2.txt',\n",
      " 'cicero/amic.txt',\n",
      " 'cicero/arch.txt',\n",
      " 'cicero/att1.txt',\n",
      " 'cicero/att10.txt',\n",
      " 'cicero/att11.txt',\n",
      " 'cicero/att12.txt',\n",
      " 'cicero/att13.txt']\n"
     ]
    }
   ],
   "source": [
    "#Filter for Cicero texts\n",
    "\n",
    "cicero = [file for file in latinlibrary.fileids() if 'cicero/' in file]\n",
    "\n",
    "\n",
    "files = cicero\n",
    "print(f\"There are {len(files)} files in the Latin Library Classical subcorpus.\")\n",
    "pprint(files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "def preprocess(text):    \n",
    "\n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = re.sub(r'&nbsp;?', ' ',text) #&nbsp; stripped incorrectly in corpus?\n",
    "    text = re.sub('\\x00',' ',text) #Another space problem?\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j\n",
    "    \n",
    "    punctuation =\"\\\"#$%&\\'()*+,-/:;<=>@[\\]^_`{|}~.?!«»\"\n",
    "    translator = str.maketrans({key: \" \" for key in punctuation})\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    translator = str.maketrans({key: \" \" for key in '0123456789'})\n",
    "    text = text.translate(translator)\n",
    "\n",
    "    remove_list = [r'\\bthe latin library\\b',\n",
    "                   r'\\bthe classics page\\b',\n",
    "                   r'\\bneo-latin\\b', \n",
    "                   r'\\bmedieval latin\\b',\n",
    "                   r'\\bchristian latin\\b',\n",
    "                   r'\\bchristina latin\\b',\n",
    "                   r'\\bpapal bulls\\b',\n",
    "                   r'\\bthe miscellany\\b',\n",
    "                  ]\n",
    "\n",
    "    for pattern in remove_list:\n",
    "        text = re.sub(pattern, '', text)\n",
    "    \n",
    "    text = re.sub('[ ]+',' ', text) # Remove double spaces\n",
    "    text = re.sub('\\s+\\n+\\s+','\\n', text) # Remove double lines and trim spaces around new lines\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make list of texts\n",
    "\n",
    "raw_files = []\n",
    "\n",
    "for file in files:\n",
    "    raw = latinlibrary.raw(file)\n",
    "    raw = preprocess(raw)\n",
    "    if len(raw) < 1000:\n",
    "        pass\n",
    "    else:\n",
    "        raw_tokens = raw.split()\n",
    "        raw = \" \".join(raw_tokens[50:-50])\n",
    "        raw_files.append(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_document(document, length):\n",
    "    segments = []\n",
    "    wordlist = document.split()\n",
    "    for i in range(0, len(wordlist), length):\n",
    "        segments.append(wordlist[i:i+length])\n",
    "    segments = [\" \".join(segment) for segment in segments]\n",
    "    if len(wordlist) % length:\n",
    "        segments[-2:] = [' '.join(segments[-2:])]\n",
    "        return segments\n",
    "    else:\n",
    "        return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = [segment_document(file, 500) for file in raw_files]\n",
    "segments = [item for sublist in segments for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "2143\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_files))\n",
    "print(len(segments))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following [Zou et al. 2006; Alajmi 2012]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make document-term matrix and vocabulary\n",
    "\n",
    "vectorizer = CountVectorizer(input='content', min_df=3)\n",
    "dtm = vectorizer.fit_transform(segments)\n",
    "dtm = dtm.toarray()\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "vocab = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27670\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(vocab)\n",
    "N= len(raw_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array of probabilities per book\n",
    "\n",
    "raw_lengths = [len(tokens.split()) for tokens in segments]\n",
    "l = np.array(raw_lengths)\n",
    "ll = l.reshape(len(l),1)\n",
    "\n",
    "probs = dtm/ll\n",
    "\n",
    "P=probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean probability\n",
    "# i.e. Sum of probabilities for each word / number of documents\n",
    "\n",
    "probsum = np.ravel(probs.sum(axis=0))\n",
    "MP = probsum/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make array of bar probability\n",
    "\n",
    "length = sum(raw_lengths)\n",
    "barprobs = dtm/length\n",
    "bP=barprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = (P-bP) ** 2\n",
    "varsum = np.ravel(variance.sum(axis=0))\n",
    "VP = varsum/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAT = MP/VP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = list(zip(vocab, MP, VP, SAT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1_data = sorted(table_data,key=lambda x: x[1], reverse=True)\n",
    "table2_data = sorted(table_data,key=lambda x: x[2], reverse=True)\n",
    "table3_data = sorted(table_data,key=lambda x: x[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['et', 'in', 'non', 'est', 'ut', 'cum', 'quod', 'ad', 'qui', 'esse']\n",
      "['and', 'in', 'is', 'not', 'when/with', 'so/how', 'at', 'which/because', 'who', 'but']\n",
      "['0.3651', '0.3195', '0.2426', '0.2076', '0.1974', '0.1596', '0.1477', '0.1374', '0.1309', '0.1289']\n",
      "['0.0103', '0.0076', '0.0046', '0.0037', '0.0032', '0.0021', '0.0019', '0.0017', '0.0016', '0.0015']\n",
      "['35.3682', '41.8374', '52.4949', '55.8152', '62.3797', '74.5576', '77.9300', '80.0493', '81.7099', '84.7584']\n"
     ]
    }
   ],
   "source": [
    "# Get Table1 info in order\n",
    "words = [item[0] for item in table1_data][:10]\n",
    "print(words)\n",
    "\n",
    "# Current output: \n",
    "# ['et', 'in', 'est', 'non', 'cum', 'ut', 'ad', 'quod', 'qui', 'sed']\n",
    "\n",
    "translations = \"and in is not when/with so/how at which/because who but\".split()\n",
    "print(translations)\n",
    "\n",
    "mps = ['{:.4f}'.format(round(item[1], 4)) for item in table1_data][:10]\n",
    "print(mps)\n",
    "\n",
    "vps = ['{:.4f}'.format(round(item[2], 4)) for item in table1_data][:10]\n",
    "print(vps)\n",
    "\n",
    "sats = ['{:.4f}'.format(round(item[3], 4)) for item in table1_data][:10]\n",
    "print(sats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1. Top 10 words with highest MP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Mean Prob.</th>\n",
       "      <th>Var. Prob.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>et</td>\n",
       "      <td>and</td>\n",
       "      <td>0.3651</td>\n",
       "      <td>0.0103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>0.3195</td>\n",
       "      <td>0.0076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non</td>\n",
       "      <td>is</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>0.0046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>est</td>\n",
       "      <td>not</td>\n",
       "      <td>0.2076</td>\n",
       "      <td>0.0037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ut</td>\n",
       "      <td>when/with</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.0032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cum</td>\n",
       "      <td>so/how</td>\n",
       "      <td>0.1596</td>\n",
       "      <td>0.0021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quod</td>\n",
       "      <td>at</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.0019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ad</td>\n",
       "      <td>which/because</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.0017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>qui</td>\n",
       "      <td>who</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>0.0016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>esse</td>\n",
       "      <td>but</td>\n",
       "      <td>0.1289</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word    Translation Mean Prob. Var. Prob.\n",
       "0    et            and     0.3651     0.0103\n",
       "1    in             in     0.3195     0.0076\n",
       "2   non             is     0.2426     0.0046\n",
       "3   est            not     0.2076     0.0037\n",
       "4    ut      when/with     0.1974     0.0032\n",
       "5   cum         so/how     0.1596     0.0021\n",
       "6  quod             at     0.1477     0.0019\n",
       "7    ad  which/because     0.1374     0.0017\n",
       "8   qui            who     0.1309     0.0016\n",
       "9  esse            but     0.1289     0.0015"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1 = [(word, translation, mp, vp) for word, translation, mp, vp, _ in zip(words, translations, mps, vps, sats)]\n",
    "df1 = pandas.DataFrame(table1, columns=['Word', 'Translation', 'Mean Prob.', 'Var. Prob.'])\n",
    "\n",
    "print(\"Table 1. Top 10 words with highest MP\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['et', 'in', 'non', 'est', 'ut', 'cum', 'quod', 'ad', 'qui', 'si']\n",
      "['and', 'in', 'is', 'not', 'when/with', 'so/how', 'at', 'which/because', 'who', 'if']\n",
      "['0.3651', '0.3195', '0.2426', '0.2076', '0.1974', '0.1596', '0.1477', '0.1374', '0.1309', '0.1282']\n",
      "['0.01032', '0.00764', '0.00462', '0.00372', '0.00316', '0.00214', '0.00189', '0.00172', '0.00160', '0.00160']\n",
      "['35.3682', '41.8374', '52.4949', '55.8152', '62.3797', '74.5576', '77.9300', '80.0493', '81.7099', '80.0770']\n"
     ]
    }
   ],
   "source": [
    "# Get Table2 info in order\n",
    "words2 = [item[0] for item in table2_data][:10]\n",
    "print(words2)\n",
    "\n",
    "# Current output: \n",
    "# ['et', 'in', 'est', 'non', 'cum', 'ut', 'ad', 'quod', 'qui', 'si']\n",
    "\n",
    "translations2 = \"and in is not when/with so/how at which/because who if\".split()\n",
    "print(translations2)\n",
    "\n",
    "mps2 = ['{:.4f}'.format(round(item[1], 4)) for item in table2_data][:10]\n",
    "print(mps2)\n",
    "\n",
    "vps2 = ['{:.5f}'.format(round(item[2], 5)) for item in table2_data][:10]\n",
    "print(vps2)\n",
    "\n",
    "sats2 = ['{:.4f}'.format(round(item[3], 4)) for item in table2_data][:10]\n",
    "print(sats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 2. Top 10 words with lowest VP\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Mean Prob.</th>\n",
       "      <th>Var. Prob.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>et</td>\n",
       "      <td>and</td>\n",
       "      <td>0.3651</td>\n",
       "      <td>0.01032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>0.3195</td>\n",
       "      <td>0.00764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non</td>\n",
       "      <td>is</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>0.00462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>est</td>\n",
       "      <td>not</td>\n",
       "      <td>0.2076</td>\n",
       "      <td>0.00372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ut</td>\n",
       "      <td>when/with</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.00316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cum</td>\n",
       "      <td>so/how</td>\n",
       "      <td>0.1596</td>\n",
       "      <td>0.00214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quod</td>\n",
       "      <td>at</td>\n",
       "      <td>0.1477</td>\n",
       "      <td>0.00189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ad</td>\n",
       "      <td>which/because</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.00172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>qui</td>\n",
       "      <td>who</td>\n",
       "      <td>0.1309</td>\n",
       "      <td>0.00160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>si</td>\n",
       "      <td>if</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.00160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word    Translation Mean Prob. Var. Prob.\n",
       "0    et            and     0.3651    0.01032\n",
       "1    in             in     0.3195    0.00764\n",
       "2   non             is     0.2426    0.00462\n",
       "3   est            not     0.2076    0.00372\n",
       "4    ut      when/with     0.1974    0.00316\n",
       "5   cum         so/how     0.1596    0.00214\n",
       "6  quod             at     0.1477    0.00189\n",
       "7    ad  which/because     0.1374    0.00172\n",
       "8   qui            who     0.1309    0.00160\n",
       "9    si             if     0.1282    0.00160"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2 = [(word, translation, mp, vp) for word, translation, mp, vp, _ in zip(words2, translations2, mps2, vps2, sats2)]\n",
    "df2 = pandas.DataFrame(table2, columns=['Word', 'Translation', 'Mean Prob.', 'Var. Prob.'])\n",
    "\n",
    "print(\"Table 2. Top 10 words with lowest VP\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['et', 'in', 'non', 'cereris', 'est', 'ut', 'sunto', 'te', 'aut', 'me']\n",
      "['and', 'in', 'is', 'not', 'when/with', 'so/how', 'at', 'which/because', 'who', 'me']\n",
      "['0.3651', '0.3195', '0.2426', '0.0003', '0.2076', '0.1974', '0.0004', '0.0837', '0.0931', '0.0852']\n",
      "['0.01032', '0.00764', '0.00462', '0.00001', '0.00372', '0.00316', '0.00001', '0.00125', '0.00133', '0.00119']\n",
      "['35.3682', '41.8374', '52.4949', '55.7312', '55.8152', '62.3797', '64.6519', '66.9224', '69.7164', '71.7725']\n"
     ]
    }
   ],
   "source": [
    "# Get Table3 info in order\n",
    "words3 = [item[0] for item in table3_data][:10]\n",
    "print(words3)\n",
    "\n",
    "# Current output: \n",
    "# ['et', 'in', 'est', 'non', 'cum', 'ut', 'ad', 'quod', 'qui', 'me']\n",
    "\n",
    "translations3 = \"and in is not when/with so/how at which/because who me\".split()\n",
    "print(translations3)\n",
    "\n",
    "mps3 = ['{:.4f}'.format(round(item[1], 4)) for item in table3_data][:10]\n",
    "print(mps3)\n",
    "\n",
    "vps3 = ['{:.5f}'.format(round(item[2], 5)) for item in table3_data][:10]\n",
    "print(vps3)\n",
    "\n",
    "sats3 = ['{:.4f}'.format(round(item[3], 4)) for item in table3_data][:10]\n",
    "print(sats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 3. Top 10 words with highest SAT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Mean Prob.</th>\n",
       "      <th>Var. Prob.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>et</td>\n",
       "      <td>and</td>\n",
       "      <td>0.3651</td>\n",
       "      <td>0.01032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>0.3195</td>\n",
       "      <td>0.00764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non</td>\n",
       "      <td>is</td>\n",
       "      <td>0.2426</td>\n",
       "      <td>0.00462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cereris</td>\n",
       "      <td>not</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>est</td>\n",
       "      <td>when/with</td>\n",
       "      <td>0.2076</td>\n",
       "      <td>0.00372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ut</td>\n",
       "      <td>so/how</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.00316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sunto</td>\n",
       "      <td>at</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.00001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>te</td>\n",
       "      <td>which/because</td>\n",
       "      <td>0.0837</td>\n",
       "      <td>0.00125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>aut</td>\n",
       "      <td>who</td>\n",
       "      <td>0.0931</td>\n",
       "      <td>0.00133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>me</td>\n",
       "      <td>me</td>\n",
       "      <td>0.0852</td>\n",
       "      <td>0.00119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word    Translation Mean Prob. Var. Prob.\n",
       "0       et            and     0.3651    0.01032\n",
       "1       in             in     0.3195    0.00764\n",
       "2      non             is     0.2426    0.00462\n",
       "3  cereris            not     0.0003    0.00001\n",
       "4      est      when/with     0.2076    0.00372\n",
       "5       ut         so/how     0.1974    0.00316\n",
       "6    sunto             at     0.0004    0.00001\n",
       "7       te  which/because     0.0837    0.00125\n",
       "8      aut            who     0.0931    0.00133\n",
       "9       me             me     0.0852    0.00119"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3 = [(word, translation, mp, vp) for word, translation, mp, vp, _ in zip(words3, translations3, mps3, vps3, sats3)]\n",
    "df3 = pandas.DataFrame(table3, columns=['Word', 'Translation', 'Mean Prob.', 'Var. Prob.'])\n",
    "\n",
    "print(\"Table 3. Top 10 words with highest SAT\")\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate entropies\n",
    "\n",
    "with np.errstate(divide='ignore', invalid='ignore'):\n",
    "    logprobs = np.where(probs != 0, np.log10(1/probs), 0)\n",
    "ent = probs * logprobs\n",
    "H = np.ravel(ent.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_data = list(zip(vocab, MP, VP, SAT, H))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "table4_data = sorted(table_data,key=lambda x: x[4], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['et', 'in', 'non', 'est', 'ut', 'cum', 'quod', 'ad', 'qui', 'esse']\n",
      "['and', 'in', 'is', 'not', 'when/with', 'so/how', 'at', 'which/because', 'who', 'me']\n"
     ]
    }
   ],
   "source": [
    "# Get Table4 info in order\n",
    "words4 = [item[0] for item in table4_data][:10]\n",
    "print(words4)\n",
    "\n",
    "# Current output: \n",
    "# ['et', 'in', 'est', 'non', 'cum', 'ut', 'ad', 'quod', 'qui', 'me']\n",
    "\n",
    "translations4 = \"and in is not when/with so/how at which/because who me\".split()\n",
    "print(translations3)\n",
    "\n",
    "#mps3 = ['{:.4f}'.format(round(item[1], 4)) for item in table3_data][:10]\n",
    "#print(mps3)\n",
    "\n",
    "#vps3 = ['{:.5f}'.format(round(item[2], 5)) for item in table3_data][:10]\n",
    "#print(vps3)\n",
    "\n",
    "#sats3 = ['{:.4f}'.format(round(item[3], 4)) for item in table3_data][:10]\n",
    "#print(sats3)\n",
    "\n",
    "ents4 = ['{:.4f}'.format(round(item[4], 4)) for item in table4_data][:10]\n",
    "#print(sats3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 4. Top 10 words with highest entropy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>et</td>\n",
       "      <td>and</td>\n",
       "      <td>79.8614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>72.8263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>non</td>\n",
       "      <td>is</td>\n",
       "      <td>58.8568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>est</td>\n",
       "      <td>not</td>\n",
       "      <td>51.6368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ut</td>\n",
       "      <td>when/with</td>\n",
       "      <td>50.0964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cum</td>\n",
       "      <td>so/how</td>\n",
       "      <td>42.2736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>quod</td>\n",
       "      <td>at</td>\n",
       "      <td>39.6297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ad</td>\n",
       "      <td>which/because</td>\n",
       "      <td>37.1852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>qui</td>\n",
       "      <td>who</td>\n",
       "      <td>35.6601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>esse</td>\n",
       "      <td>me</td>\n",
       "      <td>35.3438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Word    Translation  Entropy\n",
       "0    et            and  79.8614\n",
       "1    in             in  72.8263\n",
       "2   non             is  58.8568\n",
       "3   est            not  51.6368\n",
       "4    ut      when/with  50.0964\n",
       "5   cum         so/how  42.2736\n",
       "6  quod             at  39.6297\n",
       "7    ad  which/because  37.1852\n",
       "8   qui            who  35.6601\n",
       "9  esse             me  35.3438"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table4 = [(word, translation, ent) for word, translation, ent in zip(words4, translations4, ents4)]\n",
    "df4 = pandas.DataFrame(table4, columns=['Word', 'Translation', 'Entropy'])\n",
    "\n",
    "print(\"Table 4. Top 10 words with highest entropy\")\n",
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.stop.stop import CorpusStoplist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CorpusStoplist('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method build_stoplist in module cltk.stop.stop:\n",
      "\n",
      "build_stoplist(texts, basis='zou', size=100, sort_words=True, inc_counts=False, lower=True, remove_punctuation=True, remove_numbers=True, include=[], exclude=[]) method of cltk.stop.stop.CorpusStoplist instance\n",
      "    :param texts: list of strings used as document collection for extracting stopwords\n",
      "    :param basis: Define the basis for extracting stopwords from the corpus. Available methods are:\n",
      "                  - 'frequency', word counts\n",
      "                  - 'mean', mean probabilities\n",
      "                  - 'variance', variance probabilities\n",
      "                  - 'entropy', entropy\n",
      "                  - 'zou', composite measure as defined in the following paper\n",
      "                    Zou, F., Wang, F.L., Deng, X., Han, S., and Wang, L.S. 2006. “Automatic Construction of Chinese Stop Word List.” In Proceedings of the 5th WSEAS International Conference on Applied Computer Science, 1010–1015. https://pdfs.semanticscholar.org/c543/8e216071f6180c228cc557fb1d3c77edb3a3.pdf.\n",
      "    :param size: Set the size of the output list\n",
      "    :param sort_words: Sort output list alphabetically? (Otherwise return is descending by basis value)        \n",
      "    :param inc_counts: Include basis value; e.g. word counts for 'frequency', mean probabilities for 'mean'\n",
      "    :param lower: Lowercase corpus or no?\n",
      "    :param remove_punctuation: Remove punctuation from corpus or no?\n",
      "    :param remove_numbers: Remove numbers from corpus or no?\n",
      "    :param include: List of words in addition to stopwords that are extracted from the document collection\n",
      "                    to be added to the final list  \n",
      "    :param exclude: List of words in addition to stopwords that are extracted from the document collection\n",
      "                    to be removed from the final list \n",
      "    :type texts: list\n",
      "    :type basis: str\n",
      "    :type size: int\n",
      "    :type sort_words: bool\n",
      "    :type inc_counts: bool\n",
      "    :type lower: bool\n",
      "    :type remove_punctuation: bool\n",
      "    :type remove_numbers: bool\n",
      "    :type include: list\n",
      "    :type exclude: list\n",
      "    :return: a list of stopwords extracted from the corpus\n",
      "    :rtype: list\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(c.build_stoplist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_stops = c.build_stoplist(segments, basis='frequency', inc_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'an', 'ante', 'apud', 'atque', 'aut', 'autem', 'causa', 'cum', 'de', 'ea', 'ego', 'eius', 'enim', 'eo', 'erat', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex']\n"
     ]
    }
   ],
   "source": [
    "print(freq_stops[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table: Word Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "# from cltk.stem.latin.j_v import JVReplacer\n",
    "from cltk.stop.stop import CorpusStoplist\n",
    "\n",
    "import pickle\n",
    "\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text):\n",
    "    temp = text[1000:-1000]\n",
    "    start = temp.find(' ')\n",
    "    end = temp.rfind(' ')\n",
    "    return temp[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "import html\n",
    "# import re\n",
    "# from cltk.stem.latin.j_v import JVReplacer\n",
    "\n",
    "# replacer = JVReplacer()\n",
    "\n",
    "def preprocess(text):    \n",
    "\n",
    "#     remove_list = [r'\\bThe Latin Library\\b',\n",
    "#                    r'\\bThe Classics Page\\b',\n",
    "#                    r'\\bNeo-Latin\\b', \n",
    "#                    r'\\bIus Romanum\\b', \n",
    "#                    r'\\bMedieval Latin\\b',\n",
    "#                    r'\\bChristian Latin\\b',\n",
    "#                    r'\\bChristina Latin\\b',\n",
    "#                    r'\\bPapal Bulls\\b',\n",
    "#                    r'\\bThe Miscellany\\b',\n",
    "#                   ]\n",
    "\n",
    "#     for pattern in remove_list:\n",
    "#         text = re.sub(pattern, '', text)    \n",
    "    \n",
    "    text = html.unescape(text) # Handle html entities\n",
    "#     text = re.sub(r'&nbsp;?', ' ',text) #&nbsp; stripped incorrectly in corpus?    \n",
    "#     text = replacer.replace(text) #Normalize u/v & i/j\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_files = latinlibrary.fileids()\n",
    "ll_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in ll_files]\n",
    "ll_size = len(ll_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2164 files in the CLTK Latin Library corpus.\n"
     ]
    }
   ],
   "source": [
    "# CITED IN ARTICLE\n",
    "\n",
    "print(f'There are {ll_size} files in the CLTK Latin Library corpus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_tokens = [WordPunctTokenizer().tokenize(doc) for doc in ll_docs]\n",
    "ll_tokens = [item for sublist in ll_tokens for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16068932 tokens in the CLTK Latin Library corpus.\n",
      "There are 493557 unique tokens in the CLTK Latin Library corpus.\n",
      "Of the tokens appearing in the CLTK Latin Library corpus, 231571 tokens appear once.\n"
     ]
    }
   ],
   "source": [
    "# CITED IN ARTICLE\n",
    "\n",
    "print(f'There are {len(ll_tokens)} tokens in the CLTK Latin Library corpus.')\n",
    "print(f'There are {len(set(ll_tokens))} unique tokens in the CLTK Latin Library corpus.')\n",
    "print(f'Of the tokens appearing in the CLTK Latin Library corpus, {len([k for k, v in Counter(ll_tokens).items() if v == 1 ])} tokens appear once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "cic_files = [file for file in latinlibrary.fileids() if 'cicero/' in file]\n",
    "cic_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in cic_files]\n",
    "cic_tokens = [WordPunctTokenizer().tokenize(doc) for doc in cic_docs]\n",
    "cic_tokens = [item for sublist in cic_tokens for item in sublist] #flatten\n",
    "\n",
    "bib_files = [file for file in latinlibrary.fileids() if 'bible/' in file]\n",
    "bib_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in bib_files]\n",
    "bib_tokens = [WordPunctTokenizer().tokenize(doc) for doc in bib_docs]\n",
    "bib_tokens = [item for sublist in bib_tokens for item in sublist] #flatten\n",
    "\n",
    "ius_files = [file for file in latinlibrary.fileids() if 'justinian' in file \n",
    "                     or 'gaius' in file \n",
    "                     or 'theod' in file]\n",
    "ius_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in ius_files]\n",
    "ius_tokens = [WordPunctTokenizer().tokenize(doc) for doc in ius_docs]\n",
    "ius_tokens = [item for sublist in ius_tokens for item in sublist] #flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "colls = ['Latin Library', 'Cicero', 'Biblia Sacra', 'Ius Romanum']\n",
    "colls_abbrev = ['LL', 'LL-Cic', 'LL-Bib', 'LL-Ius']\n",
    "colls_docs = [ll_docs, cic_docs, bib_docs, ius_docs]\n",
    "colls_tokens = [ll_tokens, cic_tokens, bib_tokens, ius_tokens]\n",
    "colls_file_counts = [len(docs) for docs in colls_docs]\n",
    "colls_token_counts = [len(tokens) for tokens in colls_tokens]\n",
    "colls_unique_counts = [len(set(tokens)) for tokens in colls_tokens]\n",
    "colls_single_counts = [len([k for k, v in Counter(tokens).items() if v == 1 ]) for tokens in colls_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_419bd558_86b1_11e8_aa83_040ccee17d7c .row_heading, .blank {\n",
       "          display: none;;\n",
       "    }</style>  \n",
       "<table id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7c\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >Collections</th> \n",
       "        <th class=\"col_heading level0 col1\" >Description</th> \n",
       "        <th class=\"col_heading level0 col2\" >Files</th> \n",
       "        <th class=\"col_heading level0 col3\" >Tokens</th> \n",
       "        <th class=\"col_heading level0 col4\" >Unique Tokens</th> \n",
       "        <th class=\"col_heading level0 col5\" >Single Tokens</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7clevel0_row0\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow0_col0\" class=\"data row0 col0\" >LL</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow0_col1\" class=\"data row0 col1\" >Latin Library</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow0_col2\" class=\"data row0 col2\" >2164</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow0_col3\" class=\"data row0 col3\" >16068932</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow0_col4\" class=\"data row0 col4\" >493557</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow0_col5\" class=\"data row0 col5\" >231571</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7clevel0_row1\" class=\"row_heading level0 row1\" >1</th> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow1_col0\" class=\"data row1 col0\" >LL-Cic</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow1_col1\" class=\"data row1 col1\" >Cicero</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow1_col2\" class=\"data row1 col2\" >140</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow1_col3\" class=\"data row1 col3\" >1337074</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow1_col4\" class=\"data row1 col4\" >84611</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow1_col5\" class=\"data row1 col5\" >42386</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7clevel0_row2\" class=\"row_heading level0 row2\" >2</th> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow2_col0\" class=\"data row2 col0\" >LL-Bib</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow2_col1\" class=\"data row2 col1\" >Biblia Sacra</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow2_col2\" class=\"data row2 col2\" >77</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow2_col3\" class=\"data row2 col3\" >672413</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow2_col4\" class=\"data row2 col4\" >50420</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow2_col5\" class=\"data row2 col5\" >24426</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7clevel0_row3\" class=\"row_heading level0 row3\" >3</th> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow3_col0\" class=\"data row3 col0\" >LL-Ius</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow3_col1\" class=\"data row3 col1\" >Ius Romanum</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow3_col2\" class=\"data row3 col2\" >88</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow3_col3\" class=\"data row3 col3\" >2342252</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow3_col4\" class=\"data row3 col4\" >71689</td> \n",
       "        <td id=\"T_419bd558_86b1_11e8_aa83_040ccee17d7crow3_col5\" class=\"data row3 col5\" >29587</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x10c335c88>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sales = [('account', ['Jones LLC', 'Alpha Co', 'Blue Inc']),\n",
    "#          ('Jan', [150, 200, 50]),\n",
    "#          ('Feb', [200, 210, 90]),\n",
    "#          ('Mar', [140, 215, 95]),\n",
    "#          ]\n",
    "\n",
    "# temp = [tuple(colls)]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Collections'], df['Description'], df['Files'], df['Tokens'], df['Unique Tokens'], df['Single Tokens'] = colls_abbrev, colls, colls_file_counts, colls_token_counts, colls_unique_counts, colls_single_counts\n",
    "\n",
    "# Correct way to set up df\n",
    "# corpora = ['Latin Library', 'Cicero', 'Biblia Sacra', 'Ius Romanum']\n",
    "# sizes = [ll_size, cicero_size, biblia_sacra_size, ius_romanum_size]\n",
    "# words_sizes = [ll_words_size, cicero_words_size, biblia_sacra_words_size, ius_romanum_words_size]\n",
    "# data = {'Corpora': corpora, 'No. of Files': sizes, 'No. of Words': words_sizes}\n",
    "# pd.DataFrame.from_dict(data)\n",
    "\n",
    "df.style.set_table_styles([\n",
    "    {'selector': '.row_heading, .blank', 'props': [('display', 'none;')]}\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CorpusStoplist('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/diyclassics/Envs/stopwords-paper-pMiK5HWF/src/cltk/cltk/stop/stop.py:168: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return dtm / length_array\n"
     ]
    }
   ],
   "source": [
    "ll_freq_stops = c.build_stoplist(ll_docs, basis='frequency', inc_counts=True)\n",
    "# cic_freq_stops = c.build_stoplist(\" \".join(ll_tokens), basis='zou', inc_counts=True)\n",
    "# bib_freq_stops = c.build_stoplist(\" \".join(ll_tokens), basis='zou', inc_counts=True)\n",
    "ius_freq_stops = c.build_stoplist(ius_docs, size=25, basis='frequency', inc_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ad', 'aut', 'cj', 'conss', 'cum', 'de', 'dig', 'ed', 'esse', 'est', 'et', 'ex', 'hoc', 'id', 'in', 'non', 'quae', 'qui', 'quod', 'sed', 'si', 'sit', 'ulpianus', 'ut', 'vel']\n"
     ]
    }
   ],
   "source": [
    "print(ius_freq_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete\n",
    "# ## Get tokens for all document collections\n",
    "# ## NB: Pickled to save loading time\n",
    "\n",
    "# from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "# # ll_tokens = [WordPunctTokenizer().tokenize(doc) for doc in ll_docs]\n",
    "# # ll_tokens = [item for sublist in ll_tokens for item in sublist]\n",
    "\n",
    "# # cicero_tokens = [WordPunctTokenizer().tokenize(doc) for doc in cicero_docs]\n",
    "# # cicero_tokens = [item for sublist in cicero_tokens for item in sublist]\n",
    "\n",
    "# # biblia_sacra_tokens = [WordPunctTokenizer().tokenize(doc) for doc in biblia_sacra_docs]\n",
    "# # biblia_sacra_tokens = [item for sublist in biblia_sacra_tokens for item in sublist]\n",
    "\n",
    "# ius_romanum_tokens = [\" \".join(WordPunctTokenizer().tokenize(doc)) for doc in ius_romanum_docs]\n",
    "# # ius_romanum_tokens = [item for sublist in ius_romanum_tokens for item in sublist]\n",
    "\n",
    "# # pickle.dump(list(ll_tokens), open('../data/serial/ll_tokens.p', 'wb'))\n",
    "# # pickle.dump(list(cicero_tokens), open('../data/serial/cicero_tokens.p', 'wb'))\n",
    "# # pickle.dump(list(biblia_sacra_tokens), open('../data/serial/biblia_sacra_tokens.p', 'wb'))\n",
    "# pickle.dump(list(ius_romanum_tokens), open('../data/serial/ius_romanum_tokens.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_tokens = pickle.load(open('../data/serial/ll_tokens.p', 'rb'))\n",
    "cicero_tokens = pickle.load(open('../data/serial/cicero_tokens.p', 'rb'))\n",
    "biblia_sacra_tokens = pickle.load(open('../data/serial/biblia_sacra_tokens.p', 'rb'))\n",
    "ius_romanum_tokens = pickle.load(open('../data/serial/ius_romanum_tokens.p', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_words_size = len(ll_tokens)\n",
    "cicero_words_size = len(cicero_tokens)\n",
    "biblia_sacra_words_size = len(biblia_sacra_tokens)\n",
    "ius_romanum_words_size = len(ius_romanum_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Corpora</th>\n",
       "      <th>No. of Files</th>\n",
       "      <th>No. of Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Latin Library</td>\n",
       "      <td>2164</td>\n",
       "      <td>16764097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cicero</td>\n",
       "      <td>140</td>\n",
       "      <td>1386801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Biblia Sacra</td>\n",
       "      <td>77</td>\n",
       "      <td>697137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ius Romanum</td>\n",
       "      <td>88</td>\n",
       "      <td>2376902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Corpora  No. of Files  No. of Words\n",
       "0  Latin Library          2164      16764097\n",
       "1         Cicero           140       1386801\n",
       "2   Biblia Sacra            77        697137\n",
       "3    Ius Romanum            88       2376902"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpora = ['Latin Library', 'Cicero', 'Biblia Sacra', 'Ius Romanum']\n",
    "sizes = [ll_size, cicero_size, biblia_sacra_size, ius_romanum_size]\n",
    "words_sizes = [ll_words_size, cicero_words_size, biblia_sacra_words_size, ius_romanum_words_size]\n",
    "data = {'Corpora': corpora, 'No. of Files': sizes, 'No. of Words': words_sizes}\n",
    "pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CorpusStoplist('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-5f46c6bfc78f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# cicero_stops = c.build_stoplist(\" \".join(ll_tokens), basis='zou', inc_counts=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# biblia_sacra_stops = c.build_stoplist(\" \".join(ll_tokens), basis='zou', inc_counts=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mius_romanum_stops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_stoplist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mius_romanum_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zou'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minc_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Envs/stopwords-paper-pMiK5HWF/src/cltk/cltk/stop/stop.py\u001b[0m in \u001b[0;36mbuild_stoplist\u001b[0;34m(self, texts, basis, size, sort_words, inc_counts, lower, remove_punctuation, remove_numbers, include, exclude)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;31m# Get DTM and basic descriptive info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0mdtm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_dtm_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0mtfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_tfidf_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/stopwords-paper-pMiK5HWF/src/cltk/cltk/stop/stop.py\u001b[0m in \u001b[0;36m_make_dtm_vocab\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_dtm_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mdtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0mdtm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/stopwords-paper-pMiK5HWF/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Envs/stopwords-paper-pMiK5HWF/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    812\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# ll_stops = c.build_stoplist(\" \".join(ll_tokens), basis='zou', inc_counts=True)\n",
    "# cicero_stops = c.build_stoplist(\" \".join(ll_tokens), basis='zou', inc_counts=True)\n",
    "# biblia_sacra_stops = c.build_stoplist(\" \".join(ll_tokens), basis='zou', inc_counts=True)\n",
    "ius_romanum_stops = c.build_stoplist(\" \".join(ius_romanum_tokens), basis='zou', inc_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'atque', 'aut', 'autem', 'bellum', 'classics', 'contra', 'cum', 'de', 'dei', 'deus', 'dum', 'ea', 'ego', 'eius', 'enim', 'eo', 'erat', 'ergo', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuit', 'haec', 'hic', 'his', 'hoc', 'iam', 'id', 'illa', 'ille', 'in', 'inter', 'ipse', 'ita', 'latin', 'liber', 'library', 'livy', 'lt', 'me', 'mihi', 'modo', 'nam', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'page', 'per', 'periocha', 'post', 'pro', 'qua', 'quae', 'quam', 'quem', 'qui', 'quia', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quoque', 'res', 'se', 'secundum', 'sed', 'semper', 'si', 'sibi', 'sic', 'sicut', 'sit', 'sub', 'sunt', 'tamen', 'te', 'the', 'tibi', 'tu', 'ubi', 'urbe', 'ut', 'vel', 'vero']\n"
     ]
    }
   ],
   "source": [
    "print(ll_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'an', 'ante', 'atque', 'aut', 'autem', 'causa', 'cicero', 'ciceronis', 'cum', 'de', 'deorum', 'ea', 'ego', 'eius', 'enim', 'eo', 'erat', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuit', 'haec', 'hic', 'hoc', 'iam', 'id', 'igitur', 'illa', 'ille', 'illud', 'in', 'ipse', 'ita', 'liber', 'me', 'mihi', 'modo', 'nam', 'natura', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'omnibus', 'omnium', 'per', 'potest', 'pro', 'publica', 'publicae', 'qua', 'quae', 'quam', 'quem', 'qui', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quos', 're', 'rebus', 'rei', 'rem', 'res', 'se', 'sed', 'senatus', 'si', 'sic', 'sine', 'sit', 'sunt', 'tam', 'tamen', 'te', 'tibi', 'tu', 'tulli', 'tum', 'uel', 'uero', 'uos', 'ut']\n"
     ]
    }
   ],
   "source": [
    "print(cicero_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ad', 'ait', 'aut', 'autem', 'caput', 'christi', 'christo', 'cum', 'de', 'dei', 'deo', 'deum', 'deus', 'dicit', 'die', 'dixit', 'domine', 'domini', 'domino', 'dominum', 'dominus', 'eam', 'ecce', 'ego', 'ei', 'eis', 'eius', 'enim', 'eo', 'eorum', 'eos', 'erat', 'ergo', 'erit', 'est', 'et', 'eum', 'ex', 'filii', 'filius', 'fratres', 'haec', 'hierusalem', 'hoc', 'iesu', 'in', 'israhel', 'me', 'mea', 'meum', 'mihi', 'ne', 'nec', 'neque', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omni', 'omnia', 'omnibus', 'omnis', 'per', 'pro', 'propter', 'quae', 'quam', 'quasi', 'quem', 'qui', 'quia', 'quid', 'quis', 'quod', 'quoniam', 'rex', 'se', 'secundum', 'sed', 'si', 'sicut', 'suis', 'sum', 'sunt', 'super', 'suum', 'te', 'terra', 'terram', 'tibi', 'tu', 'tua', 'tui', 'tuum', 'uobis', 'uos', 'usque', 'ut']\n"
     ]
    }
   ],
   "source": [
    "print(biblia_sacra_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ius_romanum_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
