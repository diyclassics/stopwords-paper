{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing Stoplists for Historical Languages\n",
    "Code repository associated with the Constructing Stoplists for Historical Languages for Digital Classics Online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cltk.corpus.utils.importer import CorpusImporter\n",
    "# corpus_importer = CorpusImporter('latin')\n",
    "# corpus_importer.import_corpus('latin_text_latin_library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "from cltk.corpus.latin import latinlibrary\n",
    "from cltk.stop.latin import CorpusStoplist\n",
    "from cltk.stop.latin import PERSEUS_STOPS\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for preprocessing texts\n",
    "\n",
    "def truncate_text(text):\n",
    "    temp = text[500:-500]\n",
    "    start = temp.find(' ')\n",
    "    end = temp.rfind(' ')\n",
    "    return temp[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess texts\n",
    "\n",
    "import html\n",
    "from cltk.stem.latin.j_v import JVReplacer\n",
    "\n",
    "replacer = JVReplacer()\n",
    "\n",
    "def preprocess(text):    \n",
    "    text = html.unescape(text) # Handle html entities\n",
    "    text = replacer.replace(text) #Normalize u/v & i/j    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2152 files in the CLTK Latin Library corpus.\n"
     ]
    }
   ],
   "source": [
    "# Load CLTK Latin Library corpus; get size\n",
    "\n",
    "ll_files = latinlibrary.fileids()\n",
    "ll_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in ll_files]\n",
    "ll_docs = [doc for doc in ll_docs if len(doc) > 100]\n",
    "ll_size = len(ll_files)\n",
    "\n",
    "# CITED IN ARTICLE\n",
    "print(f'There are {ll_size} files in the CLTK Latin Library corpus.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16287634 tokens in the CLTK Latin Library corpus.\n",
      "There are 482172 unique tokens in the CLTK Latin Library corpus.\n",
      "Of the tokens appearing in the CLTK Latin Library corpus, 225612 tokens appear once.\n"
     ]
    }
   ],
   "source": [
    "# Get tokens for Latin Library; get stats\n",
    "\n",
    "ll_tokens = [WordPunctTokenizer().tokenize(doc) for doc in ll_docs]\n",
    "ll_tokens = [item for sublist in ll_tokens for item in sublist]\n",
    "\n",
    "# CITED IN ARTICLE\n",
    "\n",
    "print(f'There are {len(ll_tokens)} tokens in the CLTK Latin Library corpus.')\n",
    "print(f'There are {len(set(ll_tokens))} unique tokens in the CLTK Latin Library corpus.')\n",
    "print(f'Of the tokens appearing in the CLTK Latin Library corpus, {len([k for k, v in Counter(ll_tokens).items() if v == 1 ])} tokens appear once.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subcorpora of LL corpus\n",
    "\n",
    "# Cicero files/tokens\n",
    "cic_files = [file for file in latinlibrary.fileids() if 'cicero/' in file]\n",
    "cic_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in cic_files]\n",
    "cic_tokens = [WordPunctTokenizer().tokenize(doc) for doc in cic_docs]\n",
    "cic_tokens = [item for sublist in cic_tokens for item in sublist] #flatten\n",
    "\n",
    "# Biblia Sacra files/tokens\n",
    "bib_files = [file for file in latinlibrary.fileids() if 'bible/' in file]\n",
    "bib_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in bib_files]\n",
    "bib_tokens = [WordPunctTokenizer().tokenize(doc) for doc in bib_docs]\n",
    "bib_tokens = [item for sublist in bib_tokens for item in sublist] #flatten\n",
    "\n",
    "# Roman Legal Texts files/tokens\n",
    "ius_files = [file for file in latinlibrary.fileids() if 'justinian' in file \n",
    "                     or 'gaius' in file \n",
    "                     or 'theod' in file]\n",
    "ius_docs = [truncate_text(preprocess(latinlibrary.raw(file))) for file in ius_files]\n",
    "ius_tokens = [WordPunctTokenizer().tokenize(doc) for doc in ius_docs]\n",
    "ius_tokens = [item for sublist in ius_tokens for item in sublist] #flatten\n",
    "\n",
    "# Build a table to store the data produced above that can be imported to Pandas\n",
    "colls = ['Latin Library', 'Cicero', 'Biblia Sacra', 'Ius Romanum']\n",
    "colls_abbrev = ['LL', 'LL-Cic', 'LL-Bib', 'LL-Ius']\n",
    "colls_docs = [ll_docs, cic_docs, bib_docs, ius_docs]\n",
    "colls_tokens = [ll_tokens, cic_tokens, bib_tokens, ius_tokens]\n",
    "colls_file_counts = [len(docs) for docs in colls_docs]\n",
    "colls_token_counts = [len(tokens) for tokens in colls_tokens]\n",
    "colls_unique_counts = [len(set(tokens)) for tokens in colls_tokens]\n",
    "colls_single_counts = [len([k for k, v in Counter(tokens).items() if v == 1 ]) for tokens in colls_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_dc3ec07e_ac8c_11e8_8614_38c9863c305e .row_heading, .blank {\n",
       "          display: none;;\n",
       "    }</style>  \n",
       "<table id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305e\" > \n",
       "<thead>    <tr> \n",
       "        <th class=\"blank level0\" ></th> \n",
       "        <th class=\"col_heading level0 col0\" >Collections</th> \n",
       "        <th class=\"col_heading level0 col1\" >Description</th> \n",
       "        <th class=\"col_heading level0 col2\" >Files</th> \n",
       "        <th class=\"col_heading level0 col3\" >Tokens</th> \n",
       "        <th class=\"col_heading level0 col4\" >Unique Tokens</th> \n",
       "        <th class=\"col_heading level0 col5\" >Single Tokens</th> \n",
       "    </tr></thead> \n",
       "<tbody>    <tr> \n",
       "        <th id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305elevel0_row0\" class=\"row_heading level0 row0\" >0</th> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow0_col0\" class=\"data row0 col0\" >LL</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow0_col1\" class=\"data row0 col1\" >Latin Library</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow0_col2\" class=\"data row0 col2\" >2008</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow0_col3\" class=\"data row0 col3\" >16287634</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow0_col4\" class=\"data row0 col4\" >482172</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow0_col5\" class=\"data row0 col5\" >225612</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305elevel0_row1\" class=\"row_heading level0 row1\" >1</th> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow1_col0\" class=\"data row1 col0\" >LL-Cic</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow1_col1\" class=\"data row1 col1\" >Cicero</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow1_col2\" class=\"data row1 col2\" >138</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow1_col3\" class=\"data row1 col3\" >1361919</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow1_col4\" class=\"data row1 col4\" >84663</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow1_col5\" class=\"data row1 col5\" >42124</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305elevel0_row2\" class=\"row_heading level0 row2\" >2</th> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow2_col0\" class=\"data row2 col0\" >LL-Bib</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow2_col1\" class=\"data row2 col1\" >Biblia Sacra</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow2_col2\" class=\"data row2 col2\" >77</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow2_col3\" class=\"data row2 col3\" >684833</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow2_col4\" class=\"data row2 col4\" >50854</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow2_col5\" class=\"data row2 col5\" >24544</td> \n",
       "    </tr>    <tr> \n",
       "        <th id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305elevel0_row3\" class=\"row_heading level0 row3\" >3</th> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow3_col0\" class=\"data row3 col0\" >LL-Ius</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow3_col1\" class=\"data row3 col1\" >Ius Romanum</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow3_col2\" class=\"data row3 col2\" >88</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow3_col3\" class=\"data row3 col3\" >2360411</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow3_col4\" class=\"data row3 col4\" >71231</td> \n",
       "        <td id=\"T_dc3ec07e_ac8c_11e8_8614_38c9863c305erow3_col5\" class=\"data row3 col5\" >29197</td> \n",
       "    </tr></tbody> \n",
       "</table> "
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1537c7908>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Pandas table\n",
    "\n",
    "data = {'Collections': colls_abbrev, 'Description': colls, 'Files': colls_file_counts, 'Tokens': colls_token_counts, 'Unique Tokens': colls_unique_counts, 'Single Tokens': colls_single_counts}\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "\n",
    "df.style.set_table_styles([\n",
    "    {'selector': '.row_heading, .blank', 'props': [('display', 'none;')]}\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stoplist instance\n",
    "\n",
    "c = CorpusStoplist('latin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get frequency stops for corpora\n",
    "\n",
    "ll_freq_stops = c.build_stoplist(ll_docs, size=25, basis='frequency', inc_values=True, sort_words=False)\n",
    "cic_freq_stops = c.build_stoplist(cic_docs, size=25, basis='frequency', inc_values=True, sort_words=False)\n",
    "bib_freq_stops = c.build_stoplist(bib_docs, size=25, basis='frequency', inc_values=True, sort_words=False)\n",
    "ius_freq_stops = c.build_stoplist(ius_docs, size=25, basis='frequency', inc_values=True, sort_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LL Words</th>\n",
       "      <th>LL Freq.</th>\n",
       "      <th>LL-Cic Words</th>\n",
       "      <th>LL-Cic Freq.</th>\n",
       "      <th>LL-Bib Words</th>\n",
       "      <th>LL-Bib Freq.</th>\n",
       "      <th>LL-Ius Words</th>\n",
       "      <th>LL-Ius Freq.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>et</td>\n",
       "      <td>438829</td>\n",
       "      <td>et</td>\n",
       "      <td>25851</td>\n",
       "      <td>et</td>\n",
       "      <td>52695</td>\n",
       "      <td>et</td>\n",
       "      <td>46861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>268237</td>\n",
       "      <td>in</td>\n",
       "      <td>22593</td>\n",
       "      <td>in</td>\n",
       "      <td>22756</td>\n",
       "      <td>si</td>\n",
       "      <td>31844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>est</td>\n",
       "      <td>164895</td>\n",
       "      <td>non</td>\n",
       "      <td>17168</td>\n",
       "      <td>est</td>\n",
       "      <td>8822</td>\n",
       "      <td>non</td>\n",
       "      <td>27746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non</td>\n",
       "      <td>163873</td>\n",
       "      <td>est</td>\n",
       "      <td>14688</td>\n",
       "      <td>ad</td>\n",
       "      <td>7729</td>\n",
       "      <td>in</td>\n",
       "      <td>27310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad</td>\n",
       "      <td>131032</td>\n",
       "      <td>ut</td>\n",
       "      <td>13974</td>\n",
       "      <td>non</td>\n",
       "      <td>7642</td>\n",
       "      <td>ad</td>\n",
       "      <td>25548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ut</td>\n",
       "      <td>117020</td>\n",
       "      <td>cum</td>\n",
       "      <td>11329</td>\n",
       "      <td>qui</td>\n",
       "      <td>6988</td>\n",
       "      <td>dig</td>\n",
       "      <td>24494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>quod</td>\n",
       "      <td>102722</td>\n",
       "      <td>quod</td>\n",
       "      <td>10453</td>\n",
       "      <td>eius</td>\n",
       "      <td>5244</td>\n",
       "      <td>est</td>\n",
       "      <td>20956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cum</td>\n",
       "      <td>99167</td>\n",
       "      <td>ad</td>\n",
       "      <td>9735</td>\n",
       "      <td>autem</td>\n",
       "      <td>5174</td>\n",
       "      <td>uel</td>\n",
       "      <td>16173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>si</td>\n",
       "      <td>92659</td>\n",
       "      <td>qui</td>\n",
       "      <td>9317</td>\n",
       "      <td>de</td>\n",
       "      <td>4969</td>\n",
       "      <td>qui</td>\n",
       "      <td>14174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>qui</td>\n",
       "      <td>91813</td>\n",
       "      <td>esse</td>\n",
       "      <td>9113</td>\n",
       "      <td>ut</td>\n",
       "      <td>4714</td>\n",
       "      <td>ut</td>\n",
       "      <td>13553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>de</td>\n",
       "      <td>78294</td>\n",
       "      <td>si</td>\n",
       "      <td>9094</td>\n",
       "      <td>cum</td>\n",
       "      <td>4270</td>\n",
       "      <td>ex</td>\n",
       "      <td>13540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sed</td>\n",
       "      <td>73410</td>\n",
       "      <td>sed</td>\n",
       "      <td>8500</td>\n",
       "      <td>dominus</td>\n",
       "      <td>3511</td>\n",
       "      <td>quod</td>\n",
       "      <td>11120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quae</td>\n",
       "      <td>63085</td>\n",
       "      <td>de</td>\n",
       "      <td>8041</td>\n",
       "      <td>sunt</td>\n",
       "      <td>3469</td>\n",
       "      <td>de</td>\n",
       "      <td>10745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ex</td>\n",
       "      <td>58919</td>\n",
       "      <td>quae</td>\n",
       "      <td>7542</td>\n",
       "      <td>super</td>\n",
       "      <td>3254</td>\n",
       "      <td>cum</td>\n",
       "      <td>10222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>quam</td>\n",
       "      <td>55258</td>\n",
       "      <td>quam</td>\n",
       "      <td>7418</td>\n",
       "      <td>quae</td>\n",
       "      <td>3195</td>\n",
       "      <td>sed</td>\n",
       "      <td>10069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>per</td>\n",
       "      <td>49956</td>\n",
       "      <td>aut</td>\n",
       "      <td>6584</td>\n",
       "      <td>eum</td>\n",
       "      <td>2994</td>\n",
       "      <td>ulpianus</td>\n",
       "      <td>9779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>esse</td>\n",
       "      <td>48823</td>\n",
       "      <td>enim</td>\n",
       "      <td>6302</td>\n",
       "      <td>me</td>\n",
       "      <td>2994</td>\n",
       "      <td>ci</td>\n",
       "      <td>8888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>nec</td>\n",
       "      <td>44655</td>\n",
       "      <td>quid</td>\n",
       "      <td>6055</td>\n",
       "      <td>quod</td>\n",
       "      <td>2926</td>\n",
       "      <td>ed</td>\n",
       "      <td>8538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sunt</td>\n",
       "      <td>43471</td>\n",
       "      <td>me</td>\n",
       "      <td>6054</td>\n",
       "      <td>enim</td>\n",
       "      <td>2563</td>\n",
       "      <td>sit</td>\n",
       "      <td>8140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>hoc</td>\n",
       "      <td>43026</td>\n",
       "      <td>te</td>\n",
       "      <td>5939</td>\n",
       "      <td>israhel</td>\n",
       "      <td>2555</td>\n",
       "      <td>esse</td>\n",
       "      <td>7802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>enim</td>\n",
       "      <td>42090</td>\n",
       "      <td>etiam</td>\n",
       "      <td>5367</td>\n",
       "      <td>quia</td>\n",
       "      <td>2519</td>\n",
       "      <td>quae</td>\n",
       "      <td>7424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>uel</td>\n",
       "      <td>41193</td>\n",
       "      <td>ex</td>\n",
       "      <td>5301</td>\n",
       "      <td>te</td>\n",
       "      <td>2293</td>\n",
       "      <td>conss</td>\n",
       "      <td>6863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>se</td>\n",
       "      <td>41124</td>\n",
       "      <td>hoc</td>\n",
       "      <td>5101</td>\n",
       "      <td>eos</td>\n",
       "      <td>2122</td>\n",
       "      <td>aut</td>\n",
       "      <td>6437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>aut</td>\n",
       "      <td>40233</td>\n",
       "      <td>atque</td>\n",
       "      <td>4934</td>\n",
       "      <td>domini</td>\n",
       "      <td>2097</td>\n",
       "      <td>id</td>\n",
       "      <td>6079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>autem</td>\n",
       "      <td>40152</td>\n",
       "      <td>mihi</td>\n",
       "      <td>4263</td>\n",
       "      <td>deus</td>\n",
       "      <td>2058</td>\n",
       "      <td>hoc</td>\n",
       "      <td>5873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LL Words  LL Freq. LL-Cic Words  LL-Cic Freq. LL-Bib Words  LL-Bib Freq.  \\\n",
       "1        et    438829           et         25851           et         52695   \n",
       "2        in    268237           in         22593           in         22756   \n",
       "3       est    164895          non         17168          est          8822   \n",
       "4       non    163873          est         14688           ad          7729   \n",
       "5        ad    131032           ut         13974          non          7642   \n",
       "6        ut    117020          cum         11329          qui          6988   \n",
       "7      quod    102722         quod         10453         eius          5244   \n",
       "8       cum     99167           ad          9735        autem          5174   \n",
       "9        si     92659          qui          9317           de          4969   \n",
       "10      qui     91813         esse          9113           ut          4714   \n",
       "11       de     78294           si          9094          cum          4270   \n",
       "12      sed     73410          sed          8500      dominus          3511   \n",
       "13     quae     63085           de          8041         sunt          3469   \n",
       "14       ex     58919         quae          7542        super          3254   \n",
       "15     quam     55258         quam          7418         quae          3195   \n",
       "16      per     49956          aut          6584          eum          2994   \n",
       "17     esse     48823         enim          6302           me          2994   \n",
       "18      nec     44655         quid          6055         quod          2926   \n",
       "19     sunt     43471           me          6054         enim          2563   \n",
       "20      hoc     43026           te          5939      israhel          2555   \n",
       "21     enim     42090        etiam          5367         quia          2519   \n",
       "22      uel     41193           ex          5301           te          2293   \n",
       "23       se     41124          hoc          5101          eos          2122   \n",
       "24      aut     40233        atque          4934       domini          2097   \n",
       "25    autem     40152         mihi          4263         deus          2058   \n",
       "\n",
       "   LL-Ius Words  LL-Ius Freq.  \n",
       "1            et         46861  \n",
       "2            si         31844  \n",
       "3           non         27746  \n",
       "4            in         27310  \n",
       "5            ad         25548  \n",
       "6           dig         24494  \n",
       "7           est         20956  \n",
       "8           uel         16173  \n",
       "9           qui         14174  \n",
       "10           ut         13553  \n",
       "11           ex         13540  \n",
       "12         quod         11120  \n",
       "13           de         10745  \n",
       "14          cum         10222  \n",
       "15          sed         10069  \n",
       "16     ulpianus          9779  \n",
       "17           ci          8888  \n",
       "18           ed          8538  \n",
       "19          sit          8140  \n",
       "20         esse          7802  \n",
       "21         quae          7424  \n",
       "22        conss          6863  \n",
       "23          aut          6437  \n",
       "24           id          6079  \n",
       "25          hoc          5873  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print table\n",
    "\n",
    "data = {\n",
    "    'LL Words': list(zip(*ll_freq_stops))[0],\n",
    "    'LL Freq.': list(zip(*ll_freq_stops))[1],\n",
    "    'LL-Cic Words': list(zip(*cic_freq_stops))[0],\n",
    "    'LL-Cic Freq.': list(zip(*cic_freq_stops))[1],\n",
    "    'LL-Bib Words': list(zip(*bib_freq_stops))[0],\n",
    "    'LL-Bib Freq.': list(zip(*bib_freq_stops))[1],\n",
    "    'LL-Ius Words': list(zip(*ius_freq_stops))[0],\n",
    "    'LL-Ius Freq.': list(zip(*ius_freq_stops))[1],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.index += 1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stoplists for different bases\n",
    "\n",
    "ll_mean_stops = c.build_stoplist(ll_docs, size=100, basis='mean', inc_values=True, sort_words=False)\n",
    "ll_variance_stops = c.build_stoplist(ll_docs, size=100, basis='variance', inc_values=True, sort_words=False)\n",
    "ll_entropy_stops = c.build_stoplist(ll_docs, size=100, basis='entropy', inc_values=True, sort_words=False)\n",
    "ll_zou_stops = c.build_stoplist(ll_docs, size=25, basis='zou', inc_values=True, sort_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get relevant figures for 'zou' derived list\n",
    "\n",
    "ll_zou_words = list(zip(*ll_zou_stops))[0]\n",
    "ll_zou_mean = [round(dict(ll_mean_stops)[word], 4) for word in ll_zou_words]\n",
    "ll_zou_variance = [round(dict(ll_variance_stops)[word], 6) for word in ll_zou_words]\n",
    "ll_zou_entropy = [round(dict(ll_entropy_stops)[word], 4) for word in ll_zou_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LL 'Zou' Stopwords</th>\n",
       "      <th>Mean Prob.</th>\n",
       "      <th>Var. Prob.</th>\n",
       "      <th>Entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>et</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>93.1182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>in</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>66.4403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>est</td>\n",
       "      <td>0.0130</td>\n",
       "      <td>0.000261</td>\n",
       "      <td>46.3060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>non</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>43.3043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ad</td>\n",
       "      <td>0.0087</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>34.3664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ut</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>34.3602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cum</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>32.1202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>quod</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>30.3615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>qui</td>\n",
       "      <td>0.0066</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>27.5285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>si</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>23.4608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sed</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>24.0613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>de</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>22.2423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>quae</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>21.1070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>per</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>17.7789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>quam</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>18.2709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ex</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>17.3856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>nec</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>16.2572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>esse</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>15.1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>sunt</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>15.5193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>se</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>14.7277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>enim</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>13.5983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>hoc</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>14.7302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>autem</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>12.4945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>aut</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>12.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ab</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>13.5396</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LL 'Zou' Stopwords  Mean Prob.  Var. Prob.  Entropy\n",
       "1                  et      0.0324    0.001366  93.1182\n",
       "2                  in      0.0201    0.000503  66.4403\n",
       "3                 est      0.0130    0.000261  46.3060\n",
       "4                 non      0.0118    0.000196  43.3043\n",
       "5                  ad      0.0087    0.000112  34.3664\n",
       "6                  ut      0.0086    0.000104  34.3602\n",
       "7                 cum      0.0081    0.000107  32.1202\n",
       "8                quod      0.0077    0.000111  30.3615\n",
       "9                 qui      0.0066    0.000068  27.5285\n",
       "10                 si      0.0056    0.000060  23.4608\n",
       "11                sed      0.0056    0.000046  24.0613\n",
       "12                 de      0.0052    0.000061  22.2423\n",
       "13               quae      0.0048    0.000037  21.1070\n",
       "14                per      0.0040    0.000038  17.7789\n",
       "15               quam      0.0040    0.000024  18.2709\n",
       "16                 ex      0.0038    0.000027  17.3856\n",
       "17                nec      0.0036    0.000024  16.2572\n",
       "18               esse      0.0033    0.000024  15.1250\n",
       "19               sunt      0.0034    0.000024  15.5193\n",
       "20                 se      0.0031    0.000019  14.7277\n",
       "21               enim      0.0030    0.000019  13.5983\n",
       "22                hoc      0.0031    0.000016  14.7302\n",
       "23              autem      0.0027    0.000020  12.4945\n",
       "24                aut      0.0027    0.000016  12.9518\n",
       "25                 ab      0.0028    0.000016  13.5396"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print table\n",
    "\n",
    "data = {\n",
    "    'LL \\'Zou\\' Stopwords': ll_zou_words,\n",
    "    'Mean Prob.': ll_zou_mean,\n",
    "    'Var. Prob.': ll_zou_variance,\n",
    "    'Entropy': ll_zou_entropy,\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df.index += 1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix A: Comparison of Different Latin Stoplists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'adhic', 'aliqui', 'aliquis', 'an', 'ante', 'apud', 'at', 'atque', 'aut', 'autem', 'cum', 'cur', 'de', 'deinde', 'dum', 'ego', 'enim', 'ergo', 'es', 'est', 'et', 'etiam', 'etsi', 'ex', 'fio', 'haud', 'hic', 'iam', 'idem', 'igitur', 'ille', 'in', 'infra', 'inter', 'interim', 'ipse', 'is', 'ita', 'magis', 'modo', 'mox', 'nam', 'ne', 'nec', 'necque', 'neque', 'nisi', 'non', 'nos', 'o', 'ob', 'per', 'possum', 'post', 'pro', 'quae', 'quam', 'quare', 'qui', 'quia', 'quicumque', 'quidem', 'quilibet', 'quis', 'quisnam', 'quisquam', 'quisque', 'quisquis', 'quo', 'quoniam', 'sed', 'si', 'sic', 'sive', 'sub', 'sui', 'sum', 'super', 'suus', 'tam', 'tamen', 'trans', 'tu', 'tum', 'ubi', 'uel', 'uero', 'unus', 'ut']\n",
      "The Perseus stoplist has 92 words.\n"
     ]
    }
   ],
   "source": [
    "# Show Perseus stoplist\n",
    "\n",
    "print(PERSEUS_STOPS)\n",
    "print(f'The Perseus stoplist has {len(PERSEUS_STOPS)} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'ante', 'apud', 'atque', 'aut', 'autem', 'causa', 'cui', 'cuius', 'cum', 'de', 'dei', 'deus', 'dum', 'ea', 'ego', 'ei', 'eius', 'enim', 'eo', 'erat', 'ergo', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuit', 'haec', 'hic', 'his', 'hoc', 'iam', 'id', 'igitur', 'illa', 'ille', 'in', 'inter', 'ipse', 'ita', 'me', 'mihi', 'modo', 'nam', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'omnibus', 'per', 'post', 'potest', 'pro', 'qua', 'quae', 'quam', 'quem', 'qui', 'quia', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quoque', 'res', 'se', 'secundum', 'sed', 'si', 'sibi', 'sic', 'sicut', 'sine', 'sit', 'sub', 'sunt', 'tamen', 'te', 'tibi', 'tu', 'tunc', 'ubi', 'uel', 'uero', 'uos', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# Show 100-word LL 'Zou' stoplist\n",
    "\n",
    "ll_stops = c.build_stoplist(ll_docs, size=100, basis='zou')\n",
    "print(ll_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54 words shared by the two lists. This amounts to 58.69565217391305% of the Perseus list.\n",
      "['ab', 'ac', 'ad', 'ante', 'apud', 'atque', 'aut', 'autem', 'cum', 'de', 'dum', 'ego', 'enim', 'ergo', 'est', 'et', 'etiam', 'ex', 'hic', 'iam', 'igitur', 'ille', 'in', 'inter', 'ipse', 'ita', 'modo', 'nam', 'ne', 'nec', 'neque', 'nisi', 'non', 'nos', 'per', 'post', 'pro', 'quae', 'quam', 'qui', 'quia', 'quidem', 'quis', 'quo', 'sed', 'si', 'sic', 'sub', 'tamen', 'tu', 'ubi', 'uel', 'uero', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# Show intersection of Perseus & LL\n",
    "\n",
    "perseus_intersection = set(PERSEUS_STOPS).intersection(set(ll_stops))\n",
    "print(f'There are {len(perseus_intersection)} words shared by the two lists. This amounts to {(len(perseus_intersection)/len(PERSEUS_STOPS))*100}% of the Perseus list.')\n",
    "print(sorted(perseus_intersection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54 words from the LL list that are not found on the Perseus list.\n",
      "['causa', 'cui', 'cuius', 'dei', 'deus', 'ea', 'ei', 'eius', 'eo', 'erat', 'esse', 'esset', 'eum', 'fuit', 'haec', 'his', 'hoc', 'id', 'illa', 'me', 'mihi', 'nihil', 'nobis', 'nunc', 'omnes', 'omnia', 'omnibus', 'potest', 'qua', 'quem', 'quibus', 'quid', 'quod', 'quoque', 'res', 'se', 'secundum', 'sibi', 'sicut', 'sine', 'sit', 'sunt', 'te', 'tibi', 'tunc', 'uos']\n"
     ]
    }
   ],
   "source": [
    "# Show difference of Perseus & LL\n",
    "\n",
    "perseus_difference = set(ll_stops).difference(set(PERSEUS_STOPS))\n",
    "print(f'There are {len(perseus_intersection)} words from the LL list that are not found on the Perseus list.')\n",
    "print(sorted(perseus_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 54 words from the Perseus list that are not found on the LL list.\n",
      "['adhic', 'aliqui', 'aliquis', 'an', 'at', 'cur', 'deinde', 'es', 'etsi', 'fio', 'haud', 'idem', 'infra', 'interim', 'is', 'magis', 'mox', 'necque', 'o', 'ob', 'possum', 'quare', 'quicumque', 'quilibet', 'quisnam', 'quisquam', 'quisque', 'quisquis', 'quoniam', 'sive', 'sui', 'sum', 'super', 'suus', 'tam', 'trans', 'tum', 'unus']\n"
     ]
    }
   ],
   "source": [
    "# Show difference of Perseus & LL\n",
    "\n",
    "perseus_difference = set(PERSEUS_STOPS).difference(set(ll_stops))\n",
    "print(f'There are {len(perseus_intersection)} words from the Perseus list that are not found on the LL list.')\n",
    "print(sorted(perseus_difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ab', 'ac', 'ad', 'at', 'atque', 'aut', 'autem', 'cum', 'de', 'dum', 'e', 'erant', 'erat', 'est', 'et', 'etiam', 'ex', 'haec', 'hic', 'hoc', 'in', 'ita', 'me', 'nec', 'neque', 'non', 'per', 'qua', 'quae', 'quam', 'qui', 'quibus', 'quidem', 'quo', 'quod', 're', 'rebus', 'rem', 'res', 'sed', 'si', 'sic', 'sunt', 'tamen', 'tandem', 'te', 'ut', 'vel']\n",
      "The stopwords-json stoplist has 49 words.\n"
     ]
    }
   ],
   "source": [
    "# Show stopwords-json list\n",
    "\n",
    "json_stops = [\"a\",\"ab\",\"ac\",\"ad\",\"at\",\"atque\",\"aut\",\"autem\",\"cum\",\"de\",\"dum\",\"e\",\"erant\",\"erat\",\"est\",\"et\",\"etiam\",\"ex\",\"haec\",\"hic\",\"hoc\",\"in\",\"ita\",\"me\",\"nec\",\"neque\",\"non\",\"per\",\"qua\",\"quae\",\"quam\",\"qui\",\"quibus\",\"quidem\",\"quo\",\"quod\",\"re\",\"rebus\",\"rem\",\"res\",\"sed\",\"si\",\"sic\",\"sunt\",\"tamen\",\"tandem\",\"te\",\"ut\",\"vel\"]\n",
    "\n",
    "print(json_stops)\n",
    "print(f'The stopwords-json stoplist has {len(json_stops)} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 words shared by the two lists. This amounts to 81.63265306122449% of the stopwords-json list.\n",
      "{'autem', 'quidem', 'etiam', 'cum', 'haec', 'nec', 'sic', 'sunt', 'ad', 'te', 'quae', 'quod', 'de', 'tamen', 'sed', 'ita', 'me', 'ex', 'qua', 'hic', 'dum', 'et', 'quibus', 'est', 'quo', 'in', 'aut', 'neque', 'quam', 'erat', 'qui', 'atque', 'ab', 'ut', 'non', 'si', 'ac', 'per', 'res', 'hoc'}\n"
     ]
    }
   ],
   "source": [
    "# Show intersection of stopwords-json & LL\n",
    "\n",
    "json_intersection = set(json_stops).intersection(set(ll_stops))\n",
    "print(f'There are {len(json_intersection)} words shared by the two lists. This amounts to {(len(json_intersection)/len(json_stops))*100}% of the stopwords-json list.')\n",
    "print(json_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 words from the LL list that are not found on the stopwords-json list.\n",
      "{'eius', 'pro', 'ea', 'igitur', 'quem', 'fuit', 'tibi', 'deus', 'ergo', 'eo', 'apud', 'omnia', 'secundum', 'modo', 'sub', 'ille', 'sicut', 'ne', 'tunc', 'illa', 'nos', 'cui', 'omnes', 'ipse', 'nihil', 'causa', 'esset', 'uero', 'ego', 'uel', 'uos', 'se', 'quoque', 'cuius', 'nobis', 'nunc', 'sit', 'potest', 'enim', 'ante', 'eum', 'quia', 'esse', 'omnibus', 'inter', 'sibi', 'nam', 'id', 'tu', 'nisi', 'mihi', 'ei', 'his', 'post', 'quis', 'dei', 'ubi', 'sine', 'quid', 'iam'}\n"
     ]
    }
   ],
   "source": [
    "# Show difference of stopwords-json & LL\n",
    "\n",
    "json_difference = set(ll_stops).difference(set(json_stops))\n",
    "print(f'There are {len(json_intersection)} words from the LL list that are not found on the stopwords-json list.')\n",
    "print(json_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 40 words from the stopwords-json list that are not found on the LL list.\n",
      "{'rebus', 're', 'tandem', 'a', 'erant', 'vel', 'e', 'rem', 'at'}\n"
     ]
    }
   ],
   "source": [
    "# Show difference of stopwords-json & LL\n",
    "\n",
    "json_difference = set(json_stops).difference(set(ll_stops))\n",
    "print(f'There are {len(json_intersection)} words from the stopwords-json list that are not found on the LL list.')\n",
    "print(json_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/serial/voyant.p', 'rb') as f:\n",
    "    voyant_stops = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Voyant Tools stoplist has 4015 words.\n"
     ]
    }
   ],
   "source": [
    "print(f'The Voyant Tools stoplist has {len(voyant_stops)} words.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n",
      "{'eius', 'quidem', 'quem', 'fuit', 'tibi', 'haec', 'sunt', 'te', 'omnia', 'quod', 'tamen', 'ne', 'sed', 'tunc', 'ita', 'nos', 'omnes', 'me', 'et', 'uero', 'se', 'potest', 'omnibus', 'sibi', 'nam', 'nisi', 'ab', 'ut', 'ubi', 'apud', 'atque', 'iam', 'autem', 'cui', 'quibus', 'in', 'quid', 'aut', 'erat', 'tu', 'ei', 'his', 'post', 'sine', 'ac', 'per', 'pro', 'eo', 'sic', 'secundum', 'modo', 'illa', 'qua', 'nihil', 'causa', 'esset', 'dum', 'ego', 'quoque', 'ante', 'id', 'quam', 'qui', 'mihi', 'ea', 'etiam', 'igitur', 'cum', 'ergo', 'nec', 'ad', 'quae', 'de', 'sub', 'ille', 'sicut', 'ex', 'ipse', 'hic', 'uel', 'uos', 'est', 'cuius', 'nobis', 'quo', 'nunc', 'sit', 'enim', 'eum', 'quia', 'esse', 'inter', 'neque', 'quis', 'non', 'si', 'res', 'hoc'}\n"
     ]
    }
   ],
   "source": [
    "# Show intersection of Voyant Tools & LL\n",
    "\n",
    "voyant_intersection = set(voyant_stops).intersection(set(ll_stops))\n",
    "print(len(voyant_intersection))\n",
    "print(voyant_intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'deus', 'dei'}\n"
     ]
    }
   ],
   "source": [
    "# Show difference of Voyant Tools & LL\n",
    "\n",
    "voyant_difference = set(ll_stops).difference(set(voyant_stops))\n",
    "print(len(voyant_difference))\n",
    "print(voyant_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3848\n",
      "['', '!', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '/', '0', '1', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18', '19', '2', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '3', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '4', '40', '41', '42', '43', '44', '45', '46', '47', '48', '49', '5', '50', '51', '52', '53', '54', '55', '56', '57', '58', '59', '6', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '7', '70', '71', '72', '73', '74', '75', '76', '77', '78', '79', '8', '80', '81', '82', '83', '84', '85', '86']\n",
      "['visust', 'visuve', 'vit', 'vix', 'vobis', 'vobiscum', 'vobismet', 'vobisne', 'vobisque', 'vol', 'von', 'vop', 'vos', 'vosmet', 'vosne', 'vosque', 'voster', 'vostra', 'vostrae', 'vostraeque', 'vostram', 'vostraque', 'vostrarum', 'vostras', 'vostrast', 'vostri', 'vostris', 'vostrist', 'vostro', 'vostrorum', 'vostros', 'vostrosque', 'vostrost', 'vostrum', 'vostrumque', 'vostrumst', 'vulg', 'vv', 'w', 'x', 'xc', 'xci', 'xcii', 'xciii', 'xciv', 'xcix', 'xcv', 'xcvi', 'xcvii', 'xcviii', 'xi', 'xii', 'xiii', 'xiv', 'xix', 'xl', 'xli', 'xlii', 'xliii', 'xliv', 'xlix', 'xlv', 'xlvi', 'xlvii', 'xlviii', 'xv', 'xvi', 'xvii', 'xviii', 'xx', 'xxi', 'xxii', 'xxiii', 'xxiv', 'xxix', 'xxv', 'xxvi', 'xxvii', 'xxviii', 'xxx', 'xxxi', 'xxxii', 'xxxiii', 'xxxiv', 'xxxix', 'xxxv', 'xxxvi', 'xxxvii', 'xxxviii', 'y', 'z', '{', '|', '}', '§', '°', '¿', 'ɔ', 'γρ', '†']\n"
     ]
    }
   ],
   "source": [
    "# Show difference of Voyant Tools & LL (selection)\n",
    "\n",
    "voyant_difference = set(voyant_stops).difference(set(ll_stops))\n",
    "print(len(voyant_difference))\n",
    "print(sorted(voyant_difference)[:100])\n",
    "print(sorted(voyant_difference)[-100:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix B: 100-Word Stoplists for various Latin corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'ante', 'apud', 'atque', 'aut', 'autem', 'causa', 'cui', 'cuius', 'cum', 'de', 'dei', 'deus', 'dum', 'ea', 'ego', 'ei', 'eius', 'enim', 'eo', 'erat', 'ergo', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuit', 'haec', 'hic', 'his', 'hoc', 'iam', 'id', 'igitur', 'illa', 'ille', 'in', 'inter', 'ipse', 'ita', 'me', 'mihi', 'modo', 'nam', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'omnibus', 'per', 'post', 'potest', 'pro', 'qua', 'quae', 'quam', 'quem', 'qui', 'quia', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quoque', 'res', 'se', 'secundum', 'sed', 'si', 'sibi', 'sic', 'sicut', 'sine', 'sit', 'sub', 'sunt', 'tamen', 'te', 'tibi', 'tu', 'tunc', 'ubi', 'uel', 'uero', 'uos', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# LL 'zou' stoplist\n",
    "\n",
    "ll_stops = c.build_stoplist(ll_docs, size=100, basis='zou')\n",
    "print(ll_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ac', 'ad', 'an', 'ante', 'apud', 'atque', 'aut', 'autem', 'causa', 'cum', 'de', 'ea', 'ego', 'eius', 'enim', 'eo', 'erat', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuit', 'haec', 'hic', 'hoc', 'iam', 'id', 'igitur', 'iis', 'illa', 'ille', 'illud', 'in', 'ipse', 'is', 'ita', 'itaque', 'me', 'mihi', 'modo', 'nam', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'omnibus', 'omnium', 'per', 'potest', 'pro', 'publica', 'publicae', 'qua', 'quae', 'quam', 'quasi', 'quem', 'qui', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quos', 're', 'rebus', 'rei', 'rem', 'rerum', 'res', 'se', 'sed', 'senatus', 'si', 'sic', 'sine', 'sit', 'sunt', 'tam', 'tamen', 'te', 'tibi', 'tu', 'tum', 'uel', 'uero', 'uos', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# LL-Cic 'zou' stoplist\n",
    "\n",
    "ll_cic_stops = c.build_stoplist(cic_docs, size=100, basis='zou')\n",
    "print(ll_cic_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'ad', 'ait', 'aut', 'autem', 'caput', 'christi', 'cum', 'de', 'dei', 'deo', 'deus', 'dicit', 'die', 'dixit', 'domini', 'domino', 'dominum', 'dominus', 'domus', 'ecce', 'ego', 'ei', 'eis', 'eius', 'enim', 'eo', 'eorum', 'eos', 'erat', 'ergo', 'erit', 'est', 'estis', 'et', 'eum', 'ex', 'filii', 'filius', 'fratres', 'haec', 'hierusalem', 'his', 'hoc', 'iesu', 'in', 'ipse', 'israhel', 'me', 'mea', 'meum', 'mihi', 'ne', 'nec', 'neque', 'nobis', 'non', 'nos', 'nunc', 'omnes', 'omnia', 'omnibus', 'omnis', 'per', 'pro', 'propter', 'quae', 'quam', 'quasi', 'quem', 'qui', 'quia', 'quid', 'quis', 'quod', 'quoniam', 'rex', 'se', 'secundum', 'sed', 'si', 'sicut', 'suis', 'sum', 'sunt', 'super', 'suum', 'te', 'terra', 'terram', 'tibi', 'tu', 'tua', 'tuam', 'tui', 'tuum', 'uobis', 'uos', 'usque', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# LL-Bib 'zou' stoplist\n",
    "\n",
    "ll_bib_stops = c.build_stoplist(bib_docs, size=100, basis='zou')\n",
    "print(ll_bib_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'ab', 'ac', 'actio', 'actionem', 'ad', 'ait', 'an', 'aut', 'autem', 'causa', 'ci', 'conss', 'constantinopoli', 'contra', 'cth', 'cum', 'dat', 'de', 'debet', 'dig', 'ea', 'eam', 'ed', 'ei', 'eius', 'enim', 'eo', 'eorum', 'eos', 'erit', 'esse', 'esset', 'est', 'et', 'etiam', 'eum', 'ex', 'fuerit', 'hereditatem', 'heres', 'his', 'hoc', 'id', 'idem', 'imperator', 'imperatores', 'in', 'is', 'ita', 'iulianus', 'iure', 'ius', 'kal', 'mihi', 'nam', 'ne', 'nec', 'neque', 'nihil', 'nisi', 'nomine', 'non', 'paulus', 'per', 'posse', 'possit', 'post', 'potest', 'pp', 'pr', 'pro', 'quae', 'quam', 'qui', 'quia', 'quibus', 'quid', 'quidem', 'quis', 'quo', 'quod', 'quoque', 'rei', 'rem', 'res', 'sab', 'se', 'sed', 'si', 'sibi', 'sine', 'sit', 'siue', 'sunt', 'tamen', 'uel', 'uero', 'ulpianus', 'ut']\n"
     ]
    }
   ],
   "source": [
    "# LL-Ius 'zou' stoplist\n",
    "\n",
    "ll_ius_stops = c.build_stoplist(ius_docs, size=100, basis='zou')\n",
    "print(ll_ius_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
